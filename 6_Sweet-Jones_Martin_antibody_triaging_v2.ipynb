{"cells":[{"cell_type":"markdown","id":"679cb4ec","metadata":{"id":"679cb4ec"},"source":["# Antibody Developability Triaging Pipeline\n","\n","Implementation based on Sweet-Jones & Martin paper: https://pmc.ncbi.nlm.nih.gov/articles/PMC11901365/\n","\n","Using AntiBERTy embeddings and kernel PCA for developability assessment\n","\n","This notebook implements the methods in roughly 8 steps:\n","1. Installation & Imports\n","2. Functions for alignment of antibody sequences with ANARCI\n","3. Loading and preprocessing of the data\n","4. Embedding of the combined VH/VL regions with AntiBERTy\n","5. Unsupervised learning of the antibody latent space with Kernel PCA\n","6. Visualization of the Kernel PCA principle components\n","7. Definition of a Z-score-derived ellipse function for antibody selection\n","8. Selection of developable antibodies\n","9. Full pipeline\n","\n","### Vibe Coding\n","Code for this pipeline was initially developed using Claude Sonnet 4 and a prompt that provided a link to the manuscript with a suggested workflow for the notebook (similar to the steps above). Several fixes had to be made along the way, including addition of antibody alignment, different concatenation of the embeddings, and batching as the embeddings are generated to control for GPU memory limitations.\n","\n","Importantly, **when vibe coding, you need to check that the science is coded correctly**! Even though the pipeline runs, you will notice that this notebook does not replicate the results of the manuscript with high fidelity. In particular, the Kernel PCA plot here is fundamentally different from the one in the manuscript. Significant effort has been put into identifying the source of the discrepancy. The current thoughts are in two places. 1) the manuscript uses the AbNum server to generate Chothia numbering while this notebook utilizes ANARCI. The results of Chothia numbering with the [AbYsis server](http://www.abysis.org/) are different, altering the relative embeddings of antibodies. Alternative numbering schemes, including IMGT and Martin, have been implemented in an attempt to replicate the results, without success. 2) the manuscript documents the tokens utilized for gaps in the antibody alignment; these tokens correspond to masking tokens instead of gap/padding tokens. Tokenization differences will affect the calculation of embeddings. It is unclear if the interface to the pLMs has changed or if the authors tokenization strategy was chosen for a specific reason."]},{"cell_type":"markdown","id":"2fe5b973","metadata":{"id":"2fe5b973"},"source":["## Which csv files are used in this notebook?\n","\n","**Supplementary data**\n","Supplementary data was processed and cleaned in an effort to replicate the work in the manuscript. (See code in the section, \"Cleaning and segmenting the Supplementary Data\").\n","\n","**Updated database**\n","A second set of files was created from an up-to-date version of TheraSabDab. Here, subsets of the antibodies were created in attempts to deal with GPU memory issues. Batching fixed the problem and the full sets are available. Use a smaller set of antibodies to have a faster notebook as alignment and embedding can take some time on a large number of antibodies."]},{"cell_type":"markdown","id":"ecf40777","metadata":{"id":"ecf40777"},"source":["##### Cleaning and segmenting the Supplementary Data"]},{"cell_type":"code","execution_count":1,"id":"45f62846","metadata":{"id":"45f62846","executionInfo":{"status":"ok","timestamp":1762613000710,"user_tz":0,"elapsed":11,"user":{"displayName":"Sophia","userId":"12215712277586225497"}}},"outputs":[],"source":["####################################\n","# It is not necessary to run the code in this section, as the files are made available.\n","# Code is provided for transparency in the event you might learn from it.\n","####################################\n","\n","# import pandas as pd\n","# approved_df = pd.read_excel(\"AntibodyTriagingWithEmbeddings/kmab_a_2472009_sm7938.xlsx\", sheet_name=\"1\",skiprows=0)\n","# all_df = pd.read_csv(\"AntibodyTriagingWithEmbeddings/Sweet-Jones_clean.csv\")\n","\n","# therap_df = all_df.loc[all_df['Barcode/Name'].str.contains('umab')]\n","# therap_df.rename(columns={'Barcode/Name':'Therapeutic','VH':'HeavySequence','VL':'LightSequence'}, inplace=True)\n","# therap_df['HeavySequence'] = therap_df['HeavySequence'].str.replace(' ','')\n","# therap_df['LightSequence'] = therap_df['LightSequence'].str.replace(' ','')\n","# therap_df.loc[therap_df['Therapeutic'].isin(approved_df.loc[approved_df['Status']=='Approved','Name'].values),'Highest_Clin_Trial']='Approved'\n","# therap_df.loc[therap_df['Therapeutic'].isin(approved_df.loc[approved_df['Status']=='Discontinued','Name'].values),'Highest_Clin_Trial']='Discontinued'\n","# therap_df.loc[therap_df['Therapeutic'].isin(approved_df.loc[approved_df['Status']=='In Trials','Name'].values),'Highest_Clin_Trial']='In Trials'\n","# therap_df.loc[therap_df['Therapeutic'].isin(approved_df.loc[approved_df['Status']=='Studied','Name'].values),'Highest_Clin_Trial']='Studied'\n","# therap_df[['Therapeutic','HeavySequence','LightSequence','Highest_Clin_Trial']].to_csv('Sweet-Jones_Martin_therapeutic_mabs.csv',index=False)\n","\n","# library_df = all_df.loc[~(all_df['Barcode/Name'].str.contains('umab'))]\n","# library_df.rename(columns={'Barcode/Name':'Therapeutic','VH':'HeavySequence','VL':'LightSequence'}, inplace=True)\n","# library_df['HeavySequence'] = library_df['HeavySequence'].str.replace(' ','')\n","# library_df['LightSequence'] = library_df['LightSequence'].str.replace(' ','')\n","# library_df['Highest_Clin_Trial'] = \"Hit_Antibody\"\n","# library_df[['Therapeutic','HeavySequence','LightSequence','Highest_Clin_Trial']].to_csv('Sweet-Jones_Martin_library_mabs.csv',index=False)"]},{"cell_type":"markdown","id":"ff414a6b","metadata":{"id":"ff414a6b"},"source":["# 1. INSTALLATION & IMPORTS"]},{"cell_type":"code","execution_count":1,"id":"237e1bf3","metadata":{"id":"237e1bf3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762613081910,"user_tz":0,"elapsed":3678,"user":{"displayName":"Sophia","userId":"12215712277586225497"}},"outputId":"4b1645f8-7a5a-4aed-a40a-a49a54f87bd6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Installing dependencies...\n","‚ú®üç∞‚ú® Everything looks OK!\n"]}],"source":["if 'google.colab' in str(get_ipython()):\n","    print(\"Installing dependencies...\")\n","    !pip install -q condacolab\n","    import condacolab\n","    condacolab.install()"]},{"cell_type":"code","execution_count":2,"id":"b9b223f1","metadata":{"id":"b9b223f1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762613140149,"user_tz":0,"elapsed":54035,"user":{"displayName":"Sophia","userId":"12215712277586225497"}},"outputId":"e9259d13-4f9b-4679-db6f-db2aa22413d9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n","Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n","\n","\n","==> WARNING: A newer version of conda exists. <==\n","  current version: 24.11.2\n","  latest version: 25.9.1\n","\n","Please update conda by running\n","\n","    $ conda update -n base -c conda-forge conda\n","\n","Or to minimize the number of packages updated during conda update use\n","\n","     conda install conda=25.9.1\n","\n","\n","\n","## Package Plan ##\n","\n","  environment location: /usr/local\n","\n","  added / updated specs:\n","    - anarci\n","    - biopython\n","    - hmmer\n","\n","\n","The following packages will be downloaded:\n","\n","    package                    |            build\n","    ---------------------------|-----------------\n","    anarci-2024.05.21          |     pyhdfd78af_0         1.1 MB  bioconda\n","    biopython-1.86             |  py311h49ec1c0_0         3.2 MB  conda-forge\n","    hmmer-3.4                  |       h503566f_3        11.1 MB  bioconda\n","    ------------------------------------------------------------\n","                                           Total:        15.4 MB\n","\n","The following NEW packages will be INSTALLED:\n","\n","  anarci             bioconda/noarch::anarci-2024.05.21-pyhdfd78af_0 \n","  biopython          conda-forge/linux-64::biopython-1.86-py311h49ec1c0_0 \n","  hmmer              bioconda/linux-64::hmmer-3.4-h503566f_3 \n","\n","\n","\n","Downloading and Extracting Packages:\n","hmmer-3.4            | 11.1 MB   | :   0% 0/1 [00:00<?, ?it/s]\n","biopython-1.86       | 3.2 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n","\n","anarci-2024.05.21    | 1.1 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","anarci-2024.05.21    | 1.1 MB    | :  25% 0.2531989862149966/1 [00:00<00:00,  2.48it/s]\u001b[A\u001b[A\n","hmmer-3.4            | 11.1 MB   | :   0% 0.001409130640188588/1 [00:00<01:21, 81.92s/it]\n","\n","anarci-2024.05.21    | 1.1 MB    | : 100% 1.0/1 [00:00<00:00,  2.48it/s]               \u001b[A\u001b[A\n","hmmer-3.4            | 11.1 MB   | : 100% 1.0/1 [00:00<00:00,  4.40it/s]               \n","\n","anarci-2024.05.21    | 1.1 MB    | : 100% 1.0/1 [00:00<00:00,  2.30it/s]\u001b[A\u001b[A\n","\n","anarci-2024.05.21    | 1.1 MB    | : 100% 1.0/1 [00:00<00:00,  2.30it/s]\u001b[A\u001b[A\n","biopython-1.86       | 3.2 MB    | : 100% 1.0/1 [00:00<00:00,  1.41it/s]\u001b[A\n","                                                                        \n","                                                                        \u001b[A\n","\n","                                                                        \u001b[A\u001b[A\n","Preparing transaction: - \b\bdone\n","Verifying transaction: | \b\b/ \b\bdone\n","Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n"]}],"source":["import os\n","if 'google.colab' in str(get_ipython()):\n","\n","  # setup anarci\n","  if not os.path.exists('ANARCI_READY'):\n","    !conda install -y anarci hmmer biopython -c bioconda --no-deps --solver=classic #2>&1 1>/dev/null\n","    !touch ANARCI_READY"]},{"cell_type":"code","source":["pip install --no-dependencies antiberty"],"metadata":{"id":"ki9u4hFtu0U8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762613196789,"user_tz":0,"elapsed":13662,"user":{"displayName":"Sophia","userId":"12215712277586225497"}},"outputId":"e2fe45a8-c150-4f1f-a51c-7d3c90c3dec4"},"id":"ki9u4hFtu0U8","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting antiberty\n","  Downloading antiberty-0.1.3-py3-none-any.whl.metadata (4.7 kB)\n","Downloading antiberty-0.1.3-py3-none-any.whl (96.6 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m96.6/96.6 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: antiberty\n","Successfully installed antiberty-0.1.3\n"]}]},{"cell_type":"code","execution_count":4,"id":"3a4c65f6","metadata":{"id":"3a4c65f6","colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"status":"error","timestamp":1762613217920,"user_tz":0,"elapsed":14123,"user":{"displayName":"Sophia","userId":"12215712277586225497"}},"outputId":"c0c86aee-db16-42f0-c802-69a26d9db507"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running on Google Colab. Executing Colab-specific commands...\n"]},{"output_type":"error","ename":"MessageError","evalue":"Error: credential propagation was unsuccessful","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4-1624204733.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Mount Google Drive to access files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Drive location for the fasta files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["if 'google.colab' in str(get_ipython()):\n","    print(\"Running on Google Colab. Executing Colab-specific commands...\")\n","    # Mount Google Drive to access files\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    # Drive location for the fasta files\n","    data_loc = '/content/drive/MyDrive/AIDrivenDesignOfBiologics/AIDrivenDesignOfBiologics-PEGSEurope-2025/ProteinLanguageModels/AntibodyTriagingWithEmbeddings/'\n","\n","else:\n","    print(\"Not running on Google Colab. Skipping Colab-specific commands.\")\n","    print(\"Running in a local environment or Jupyter Notebook.\")\n","    data_loc = '/home/davidnannemann/AIDD4B/ProteinLMs/'"]},{"cell_type":"code","source":["if 'google.colab' in str(get_ipython()):\n","    therapeutic_antibodies_csv = f'{data_loc}/Sweet-Jones_Martin_therapeutic_mabs.csv'\n","    random_paired_antibodies_csv = f'{data_loc}/Sweet-Jones_Martin_library_mabs.csv'\n","else:\n","    therapeutic_antibodies_csv = 'AntibodyTriagingWithEmbeddings/Sweet-Jones_Martin_therapeutic_mabs.csv'\n","    random_paired_antibodies_csv = 'AntibodyTriagingWithEmbeddings/Sweet-Jones_Martin_library_mabs.csv'\n","\n","    #therapeutic_antibodies_csv = 'AntibodyTriagingWithEmbeddings/therapeutic_antibodies_approved.csv'\n","    #random_paired_antibodies_csv = 'AntibodyTriagingWithEmbeddings/paired_sequences_10k.csv'"],"metadata":{"id":"YbVuZs2vxUZm"},"id":"YbVuZs2vxUZm","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"0abded6c","metadata":{"id":"0abded6c"},"outputs":[],"source":["if not 'google.colab' in str(get_ipython()):\n","    import os\n","    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","\n","import pandas as pd\n","import numpy as np\n","import string\n","import gc\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.decomposition import KernelPCA\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics.pairwise import rbf_kernel\n","import torch\n","from transformers import AutoTokenizer, AutoModel\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","pd.set_option('display.width', 250) # Auto-detect width in terminal\n","pd.set_option('display.max_colwidth', 200) # Limit column content to 100 characters\n","\n","# AntiBERTy for antibody embeddings\n","try:\n","    from antiberty import AntiBERTyRunner\n","    ANTIBERTY_AVAILABLE = True\n","    print(\"AntiBERTy imported successfully!\")\n","except ImportError:\n","    ANTIBERTY_AVAILABLE = False\n","    print(\"WARNING: AntiBERTy not available. Please install with: pip install antiberty\")\n","\n","# ANARCI for antibody numbering and alignment\n","try:\n","    from anarci import anarci\n","    from anarci.anarci import run_anarci\n","    ANARCI_AVAILABLE = True\n","    print(\"ANARCI imported successfully!\")\n","except ImportError:\n","    ANARCI_AVAILABLE = False\n","    print(\"WARNING: ANARCI not available. Please install with: pip install anarci\")\n","    print(\"You may also need to install HMMER and configure ANARCI databases.\")\n","\n","# Set style for plots\n","plt.style.use('seaborn-v0_8')\n","sns.set_palette(\"husl\")\n","\n","print(\"Imports completed successfully!\")\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"AntiBERTy available: {ANTIBERTY_AVAILABLE}\")\n","print(f\"ANARCI available: {ANARCI_AVAILABLE}\")"]},{"cell_type":"code","execution_count":null,"id":"da65936c","metadata":{"id":"da65936c"},"outputs":[],"source":["# INSTALLATION CHECKS\n","def check_anarci_installation():\n","    \"\"\"\n","    Check ANARCI installation and provide installation instructions.\n","    \"\"\"\n","\n","    print(\"=\"*60)\n","    print(\"ANARCI INSTALLATION CHECK\")\n","    print(\"=\"*60)\n","\n","    if ANARCI_AVAILABLE:\n","        print(\"‚úì ANARCI is installed and available\")\n","        try:\n","            # Test basic functionality\n","            test_seq = \"QVQLVQSGAEVKKPGASVKVSCKASGYTFTSYAMHWVRQAPGQGLEWMGWINAGNGNTKYSQKFQGRVTITRDTSASTAYMELSSLRSEDTAVYYCAR\"\n","            result = anarci([('test', test_seq)], scheme='chothia', output=False)\n","            if result and result[0]:\n","                print(\"‚úì ANARCI test run successful\")\n","                return True\n","            else:\n","                print(\"‚ö† ANARCI installed but test run failed\")\n","                return False\n","        except Exception as e:\n","            print(f\"‚ö† ANARCI installed but error during test: {e}\")\n","            return False\n","    else:\n","        print(\"‚úó ANARCI is not available\")\n","        print(\"\\nTo install ANARCI:\")\n","        print(\"1. Install dependencies:\")\n","        print(\"   conda install -c bioconda hmmer\")\n","        print(\"   # OR\")\n","        print(\"   sudo apt-get install hmmer  # Ubuntu/Debian\")\n","        print(\"   # OR\")\n","        print(\"   brew install hmmer  # MacOS\")\n","        print(\"\\n2. Install ANARCI:\")\n","        print(\"   pip install anarci\")\n","        print(\"\\n3. Setup ANARCI database (run once):\")\n","        print(\"   python -c 'import anarci; anarci.setup_database()'\")\n","        print(\"\\nAlternative installation via conda:\")\n","        print(\"   conda install -c bioconda anarci\")\n","        return False\n","\n","def check_antiberty_installation():\n","    \"\"\"\n","    Check AntiBERTy installation and provide installation instructions.\n","    \"\"\"\n","\n","    print(\"=\"*60)\n","    print(\"ANTIBERTY INSTALLATION CHECK\")\n","    print(\"=\"*60)\n","\n","    if ANTIBERTY_AVAILABLE:\n","        print(\"‚úì AntiBERTy is installed and available\")\n","        return True\n","    else:\n","        print(\"‚úó AntiBERTy is not available\")\n","        print(\"\\nTo install AntiBERTy:\")\n","        print(\"  pip install antiberty\")\n","        return False\n","def check_all_dependencies():\n","    \"\"\"\n","    Check all required dependencies and provide installation instructions.\n","    \"\"\"\n","\n","    print(\"=\"*70)\n","    print(\"DEPENDENCY CHECK FOR ANTIBODY DEVELOPABILITY PIPELINE\")\n","    print(\"=\"*70)\n","\n","    all_ready = True\n","\n","    # Check AntiBERTy\n","    antiberty_ready = check_antiberty_installation()\n","    all_ready = all_ready and antiberty_ready\n","\n","    print(\"\\n\" + \"=\"*60)\n","\n","    # Check ANARCI\n","    anarci_ready = check_anarci_installation()\n","    all_ready = all_ready and anarci_ready\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"OVERALL STATUS\")\n","    print(\"=\"*60)\n","\n","    if all_ready:\n","        print(\"‚úì All dependencies are ready!\")\n","        print(\"You can run the full pipeline with sequence alignment and AntiBERTy embeddings.\")\n","    else:\n","        print(\"‚ö† Some dependencies are missing.\")\n","        if antiberty_ready and not anarci_ready:\n","            print(\"You can run the pipeline without sequence alignment.\")\n","        elif anarci_ready and not antiberty_ready:\n","            print(\"You need AntiBERTy for embedding extraction.\")\n","        else:\n","            print(\"Please install the missing dependencies before running the pipeline.\")\n","\n","    return all_ready, antiberty_ready, anarci_ready\n","\n","all_ready, antiberty_ready, anarci_ready = check_all_dependencies()\n","\n","print(f\"AntiBERTy ready: {antiberty_ready}\")\n","print(f\"ANARCI ready: {anarci_ready}\")\n","print(\"All ready: \", all_ready)"]},{"cell_type":"markdown","id":"b5daf139","metadata":{"id":"b5daf139"},"source":["# 2. ANTIBODY SEQUENCE ALIGNMENT USING ANARCI\n","\n","Human antibody variable regions have a consistent structure:\n","- one heavy chain domani and one light chain domain\n","- Three CDRs and four Framework Regions (FWR) per domain\n","\n","Regression strategies can benefit from providing the embeddings in a consistent structure across the samples. We do this by aligning the antibodies with an antibody-specific alignment tool called ANARCI.\n","\n","Below is a script that uses ANARCI to number each antibody sequence (using Chothia numbering, though everyone knows IMGT is better ;-) ), and create a classical alignment structure based on that numbering."]},{"cell_type":"code","execution_count":null,"id":"dc10ce05","metadata":{"id":"dc10ce05"},"outputs":[],"source":["heavy_chothia_scheme = \"\"\"1     2     3     4     5     6     7     8     9\n","          10    11    12    13    14    15    16    17    18    19\n","          20    21    22    23    24    25    26    27    28    29\n","          30    31\n","          31A   31B\n","          32    33    34    35    36    37    38    39\n","          40    41    42    43    44    45    46    47    48    49\n","          50    51    52\n","          52A   52B   52C   53    54    55    56    57    58    59\n","          60    61    62    63    64    65    66    67    68    69\n","          70    71    72    73    74    75    76    77    78    79\n","          80    81    82\n","          82A   82B   82C   83    84    85    86    87    88    89\n","          90    91    92    93    94    95    96    97    98    99\n","          100\n","          100A  100B  100C  100D  100E  100F  100G  100H  100I  100J\n","          100K  101   102   103   104   105   106   107   108   109\n","          110   111   112   113\"\"\"\n","# 100L 100M 100N 100O 100P 100Q\n","light_chothia_scheme = \"\"\"1     2     3     4     5     6     7     8     9\n","          10    11    12    13    14    15    16    17    18    19\n","          20    21    22    23    24    25    26    27    28    29\n","          30\n","          30A   30B   30C   30D   30E   30F\n","          31    32    33    34    35    36    37    38    39\n","          40    41    42    43    44    45    46    47    48    49\n","          50    51    52    53    54    55    56    57    58    59\n","          60    61    62    63    64    65    66    67    68    69\n","          70    71    72    73    74    75    76    77    78    79\n","          80    81    82    83    84    85    86    87    88    89\n","          90    91    92    93    94    95\n","          95A   95B   95C   95D   95E   95F   96    97    98    99\n","          100   101   102   103   104   105   106\n","          106A                                      107   108   109\"\"\"\n","# '52A', '52B', '52C', '52D'\n","\n","heavy_martin_scheme = \"\"\"1     2     3     4     5     6     7     8\n","          8A    8B    8C    8D                                   9\n","          10    11    12    13    14    15    16    17    18    19\n","          20    21    22    23    24    25    26    27    28    29\n","          30    31\n","          31A   31B   31C   31D   31E   31F   31G   31H   31I   31J   31K\n","          32    33    34    35    36    37    38    39\n","          40    41    42    43    44    45    46    47    48    49\n","          50    51    52\n","          52A   52B   52C   52D   52E   52F   52G   52H   52I   52J   52K   52L\n","          53    54    55    56    57    58    59\n","          60    61    62    63    64    65    66    67    68    69\n","          70    71    72\n","          72A   72B   72C   72D   72E   72F   72G   72H   72I\n","                            73    74    75    76    77    78    79\n","          80    81    82    83    84    85    86    87    88    89\n","          90    91    92    93    94    95    96    97    98    99\n","          100\n","          100A  100B  100C  100D  100E  100F  100G  100H  100I  100J\n","          100K  100L  100M  100N  100O  100P  100Q  100R  100S  100T 100U 100V 100W 100X 100Y 100Z\n","          101   102   103   104   105   106   107   108   109\n","          110   111   112   113\"\"\"\n","\n","light_martin_scheme = \"\"\"1     2     3     4     5     6     7     8     9\n","          10    11    12    13    14    15    16    17    18    19\n","          20    21    22    23    24    25    26    27    28    29\n","          30\n","          30A   30B   30C   30D   30E   30F   30G   30H   30I   30J   30K\n","          31    32    33    34    35    36    37    38    39\n","          40\n","          40A   41    42    43    44    45    46    47    48    49\n","          50    51    52\n","          52A   52B   52C   52D   52E\n","          53    54    55    56    57    58    59\n","          60    61    62    63    64    65    66    67    68\n","          68A   68B   68C   68D   68E   68F   68G   68H         69\n","          70    71    72    73    74    75    76    77    78    79\n","          80    81    82    83    84    85    86    87    88    89\n","          90    91    92    93    94    95\n","          95A   95B   95C   95D   95E   95F   95G   95H   95I\n","          96    97    98    99\n","          100   101   102   103   104   105   106   107\n","          107A                                            108   109\n","          110\"\"\"\n","\n","expanded_imgt_scheme       = \"\"\"1     2     3     4     5\n","                 5A    5B    5C    5D                6     7     8     9\n","                10    11    12    13    14    15    16    17    18    19\n","                20    21    22    23\n","                23A                     24    25    26    27    28    29\n","                30    31    32\n","                32A   32B\n","                33B   33A         33    34    35    36    37    38    39\n","                40    41    42    43    44    45\n","                45A                                 46\n","                46A   46B   46C   46D   46E   46F         47    48    49\n","                50    50A   50B   50C   50D   50E\n","                      51    52    52A   52B   52C   52D\n","                                  53    54    55    56    57    58    59\n","                60    60A   60B   60C   60D   60E   60F\n","                      61F   61E   61D   61C   61B   61A\n","                      61    62    63    64    65    66    67\n","                67A   67B   67C   67D   67E   67F\n","                68A                                             68    69\n","                69A\n","                70    70A   70B   70C\n","                      71    72    73    74    75    76    77    78    79\n","                80    80A   80B\n","                      81    82    82A   82B   82C   82D   82E\n","                                  83    84    84A   84B\n","                                              85    86    87    88    89\n","                89A   89B\n","                90    91    92    93    94    95    96    97    98    99\n","               100   101   102   103   104   105   106   107   108   109\n","               110   111\n","               111A  111B  111C  111D  111E  111F  111G  111H  111I  111J  111K  111L  111M\n","               112M  112L  112K  112J  112I  112H  112G  112F  112E  112D  112C  112B  112A\n","                           112   113   114   115   116   117   118   119   119A  119B  119C\n","               120   121   122   123   124   125   126   127   128\n","               \"\"\"\n","basic_imgt_scheme = \"\"\"1     2     3     4     5     6     7     8     9\n","                10    11    12    13    14    15    16    17    18    19\n","                20    21    22    23    24    25    26    27    28    29\n","                30    31    32\n","                32A   32B\n","                33B   33A         33    34    35    36    37    38    39\n","                40    41    42    43    44    45    46    47    48    49\n","                50    51    52    53    54    55    56    57    58    59\n","                60    60A   60B   60C   60D   60E   60F\n","                      61F   61E   61D   61C   61B   61A\n","                      61    62    63    64    65    66    67    68    69\n","                70    71    72    73    74    75    76    77    78    79\n","                80    81    82    83    84    85    86    87    88    89\n","                90    91    92    93    94    95    96    97    98    99\n","               100   101   102   103   104   105   106   107   108   109\n","               110   111\n","               111A  111B  111C  111D  111E  111F  111G  111H  111I  111J  111K  111L  111M\n","               112M  112L  112K  112J  112I  112H  112G  112F  112E  112D  112C  112B  112A\n","                           112   113   114   115   116   117   118   119\n","               120   121   122   123   124   125   126   127   128\n","               \"\"\"\n","\n","imgt_scheme = expanded_imgt_scheme.split()\n","heavy_chothia_scheme = heavy_chothia_scheme.split()\n","heavy_martin_scheme = heavy_martin_scheme.split()\n","\n","heavy_numbering_scheme = heavy_martin_scheme\n","print(len(heavy_numbering_scheme), heavy_numbering_scheme)\n","\n","light_chothia_scheme = light_chothia_scheme.split()\n","light_martin_scheme = light_martin_scheme.split()\n","light_numbering_scheme = light_martin_scheme\n","print(len(light_numbering_scheme), light_numbering_scheme)"]},{"cell_type":"code","execution_count":null,"id":"6cf12767","metadata":{"id":"6cf12767"},"outputs":[],"source":["class AntibodyAligner:\n","    \"\"\"\n","    Class for aligning antibody sequences using ANARCI with specified numbering scheme.\n","    \"\"\"\n","\n","    def __init__(self, scheme='martin', allowed_species=['human']):\n","        \"\"\"\n","        Initialize the antibody aligner.\n","\n","        Args:\n","            scheme (str): Numbering scheme ('chothia', 'kabat', 'imgt', 'martin')\n","            allowed_species (list): Allowed species for recognition\n","        \"\"\"\n","        if not ANARCI_AVAILABLE:\n","            raise ImportError(\"ANARCI is required for sequence alignment. Please install with: pip install anarci\")\n","\n","        self.scheme = scheme\n","        self.allowed_species = allowed_species\n","\n","        # Define standard positions for alignment (with specified numbering scheme)\n","        # Heavy chain positions: 1-113 (approximate, varies by CDR insertions)\n","        # Light chain positions: 1-107 (approximate, varies by CDR insertions)\n","        self.heavy_positions = None\n","        self.light_positions = None\n","\n","        self.failed_indices = None\n","\n","    def number_sequence(self, sequence, chain_type='H'):\n","        \"\"\"\n","        Number a single antibody sequence using ANARCI.\n","\n","        Args:\n","            sequence (str): Antibody sequence\n","            chain_type (str): 'H' for heavy, 'L' for light\n","\n","        Returns:\n","            tuple: (numbered_sequence, domain_type, species)\n","        \"\"\"\n","\n","        try:\n","            # Run ANARCI numbering\n","            results = anarci([('query', sequence)],\n","                           scheme=self.scheme,\n","                           output=False,\n","                           allowed_species=self.allowed_species)\n","\n","            if not results or not results[0] or not results[0][0]:\n","                return None, None, None\n","\n","            # Extract results\n","            numbering, alignment_details, hit_table = results\n","\n","            if not numbering[0] or not numbering[0][0]:\n","                return None, None, None\n","\n","            # Get the first (best) result\n","            domain_numbering = numbering[0][0]\n","            details = alignment_details[0]\n","\n","            domain_type = details['query_name'] if 'query_name' in details else 'unknown'\n","            species = details['species'] if 'species' in details else 'unknown'\n","\n","            return domain_numbering, domain_type, species\n","\n","        except Exception as e:\n","            print(f\"Error numbering sequence: {e}\")\n","            return None, None, None\n","\n","    def create_aligned_sequence(self, numbering, standard_positions):\n","        \"\"\"\n","        Create aligned sequence with gaps for missing positions.\n","\n","        Args:\n","            numbering (list): ANARCI numbering results\n","            standard_positions (list): Standard positions for alignment\n","\n","        Returns:\n","            str: Aligned sequence with gaps\n","        \"\"\"\n","\n","        if not numbering:\n","            return None\n","\n","        # Create position to residue mapping\n","        pos_to_residue = {}\n","        for position, residue in numbering[0]:\n","            if residue != '-':  # Skip gaps in the original numbering\n","                pos_to_residue[position] = residue\n","\n","        # Build aligned sequence\n","        aligned_seq = []\n","        for pos in standard_positions:\n","            if pos in pos_to_residue:\n","                aligned_seq.append(pos_to_residue[pos])\n","            else:\n","                aligned_seq.append('-')  # Gap for missing position\n","\n","        return ''.join(aligned_seq)\n","\n","    def get_standard_positions(self, all_numberings, chain_type='H'):\n","        \"\"\"\n","        Determine standard positions from all numbered sequences.\n","\n","        Args:\n","            all_numberings (list): List of numbering results\n","            chain_type (str): 'H' for heavy, 'L' for light\n","\n","        Returns:\n","            list: Sorted list of standard positions\n","        \"\"\"\n","\n","        all_positions = set()\n","\n","        for numbering in all_numberings:\n","            if numbering: # Check if numbering was successful\n","                for position, residue in numbering[0]:\n","                    if residue != '-': # Ignore gaps from ANARCI\n","                        all_positions.add(position)\n","\n","        # Sort positions (ANARCI positions are tuples like (1, ' ') or (27, 'A'))\\\n","        if chain_type==\"H\":\n","            numbering_scheme = heavy_numbering_scheme\n","        if chain_type==\"L\":\n","            numbering_scheme = light_numbering_scheme\n","        sorted_positions = []\n","        unsorted_positions = []\n","\n","        used_positions = [str(y[0])+y[1].strip() for y in all_positions]\n","        for numstring in numbering_scheme:\n","            if numstring in used_positions:\n","                if numstring[-1] in string.ascii_uppercase:\n","                    sorted_positions.append((int(numstring[:-1]),numstring[-1]))\n","                else:\n","                    sorted_positions.append((int(numstring),' '))\n","            else:\n","                unsorted_positions.append(numstring)\n","\n","        if len(unsorted_positions)>0:\n","            print(f\"WARNING: The following positions are not accounted for {unsorted_positions}\")\n","        return sorted_positions\n","\n","    def align_antibody_sequences(self, df, heavy_col='HeavySequence', light_col='LightSequence'):\n","        \"\"\"\n","        Align all antibody sequences in the dataframe using Chothia numbering.\n","\n","        Args:\n","            df (pd.DataFrame): DataFrame with antibody sequences\n","            heavy_col (str): Column name for heavy chain sequences\n","            light_col (str): Column name for light chain sequences\n","\n","        Returns:\n","            pd.DataFrame: DataFrame with aligned sequences added\n","        \"\"\"\n","\n","        print(f\"Aligning {len(df)} antibody sequences using ANARCI ({self.scheme} numbering)...\")\n","\n","        df_aligned = df.copy()\n","\n","        # Initialize lists for results\n","        heavy_aligned = []\n","        light_aligned = []\n","        heavy_numberings = []\n","        light_numberings = []\n","        alignment_success = []\n","\n","        # First pass: Number all sequences\n","        print(\"First pass: Numbering sequences...\")\n","        for idx, row in df.iterrows():\n","            if idx % 500 == 0:\n","                print(f\"Processing antibody {idx+1}/{len(df)}\")\n","\n","            # Number heavy chain\n","            heavy_numbering, heavy_domain, heavy_species = self.number_sequence(\n","                row[heavy_col], chain_type='H'\n","            )\n","\n","            # Number light chain\n","            light_numbering, light_domain, light_species = self.number_sequence(\n","                row[light_col], chain_type='L'\n","            )\n","\n","            if row[heavy_col] in numbering_corrections:\n","                heavy_numbering = numbering_corrections[row[heavy_col]]\n","\n","            #print(heavy_numbering)\n","            #print([x[0] for x in heavy_numbering[0]])\n","            extra_heavy_num = [x for x in [str(y[0][0])+y[0][1].strip() for y in heavy_numbering[0]] if x not in heavy_numbering_scheme]\n","            extra_light_num = [x for x in [str(y[0][0])+y[0][1].strip() for y in light_numbering[0]] if x not in light_numbering_scheme]\n","\n","            if len(extra_heavy_num)>0:\n","                print(row['Therapeutic'], row[heavy_col])\n","                print(extra_heavy_num)\n","                print(heavy_numbering)\n","                heavy_numbering = None\n","\n","            if len(extra_light_num)>0:\n","                print(row['Therapeutic'], row[light_col])\n","                print(extra_light_num)\n","                print(light_numbering)\n","                light_numbering = None\n","\n","\n","            heavy_numberings.append(heavy_numbering)\n","            light_numberings.append(light_numbering)\n","\n","            # Track success\n","            success = (heavy_numbering is not None) and (light_numbering is not None)\n","            alignment_success.append(success)\n","\n","        # Determine standard positions from all successful numberings\n","        print(\"Determining standard positions...\")\n","        successful_heavy = [n for n, s in zip(heavy_numberings, alignment_success) if s and n]\n","        successful_light = [n for n, s in zip(light_numberings, alignment_success) if s and n]\n","\n","        self.heavy_positions = self.get_standard_positions(successful_heavy, 'H')\n","        self.light_positions = self.get_standard_positions(successful_light, 'L')\n","\n","        print(f\"Heavy chain alignment length: {len(self.heavy_positions)} positions\")\n","        print(f\"Light chain alignment length: {len(self.light_positions)} positions\")\n","\n","        # Second pass: Create aligned sequences\n","        print(\"Second pass: Creating aligned sequences...\")\n","        for i, (heavy_num, light_num, success) in enumerate(zip(heavy_numberings, light_numberings, alignment_success)):\n","            if success:\n","                heavy_aligned_seq = self.create_aligned_sequence(heavy_num, self.heavy_positions)\n","                light_aligned_seq = self.create_aligned_sequence(light_num, self.light_positions)\n","            else:\n","                heavy_aligned_seq = None\n","                light_aligned_seq = None\n","\n","            heavy_aligned.append(heavy_aligned_seq)\n","            light_aligned.append(light_aligned_seq)\n","\n","        # Add aligned sequences to dataframe\n","        df_aligned['HeavyAligned'] = heavy_aligned\n","        df_aligned['LightAligned'] = light_aligned\n","        df_aligned['AlignmentSuccess'] = alignment_success\n","\n","        # Summary statistics\n","        successful_alignments = sum(alignment_success)\n","        print(f\"\\nAlignment Results:\")\n","        print(f\"Successfully aligned: {successful_alignments}/{len(df)} ({100*successful_alignments/len(df):.1f}%)\")\n","\n","        if successful_alignments < len(df):\n","            self.failed_indices = [i for i, s in enumerate(alignment_success) if not s]\n","            print(f\"Failed alignments at indices: {self.failed_indices[:10]}{'...' if len(self.failed_indices) > 10 else ''}\")\n","\n","        return df_aligned\n","\n","    def get_alignment_info(self):\n","        \"\"\"Get information about the current alignment.\"\"\"\n","        return {\n","            'scheme': self.scheme,\n","            'heavy_positions': len(self.heavy_positions) if self.heavy_positions else 0,\n","            'light_positions': len(self.light_positions) if self.light_positions else 0,\n","            'heavy_pos_range': f\"{self.heavy_positions[0]}-{self.heavy_positions[-1]}\" if self.heavy_positions else \"None\",\n","            'light_pos_range': f\"{self.light_positions[0]}-{self.light_positions[-1]}\" if self.light_positions else \"None\"\n","        }"]},{"cell_type":"markdown","id":"8af9337d","metadata":{"id":"8af9337d"},"source":["# 3. DATA LOADING AND PREPROCESSING\n","\n","We're going to read and concatenate the two csv files with antibody sequences and clinical progression tags, separating out the antibodies that are approved and in the clinic. A different separation strategy could provide a better re-implementation of the original manuscript. Think about this if you want."]},{"cell_type":"code","execution_count":null,"id":"37589e90","metadata":{"id":"37589e90"},"outputs":[],"source":["numbering_corrections = {}\n","\n","def load_antibody_data(csv_file_path, random_paired_file_path):\n","    \"\"\"\n","    Load therapeutic antibody data from CSV file.\n","\n","    Expected columns:\n","    - Therapeutic: Antibody name\n","    - HeavySequence: Heavy chain sequence\n","    - LightSequence: Light chain sequence\n","    - Highest_Clin_Trial: Clinical trial status\n","    \"\"\"\n","\n","    # Load the data\n","    df = pd.read_csv(csv_file_path)\n","\n","    # remove duplicates\n","    num_seqs_begin = df.shape[0]\n","    df = df.drop_duplicates(subset=['HeavySequence','LightSequence'])\n","    num_seqs_end = df.shape[0]\n","\n","    # select only \"human\" sequences\n","    # umab_mask  = df['Therapeutic'].str.contains('umab', case=False, na=False)\n","    # zumab_mask = df['Therapeutic'].str.contains('zumab', case=False, na=False)\n","    # df = df.loc[umab_mask & ~zumab_mask]\n","\n","    # Display basic information about the dataset\n","    print(f\"Loaded {len(df)} antibodies from {csv_file_path}\")\n","    print(f\"Removed {num_seqs_begin - num_seqs_end} antibodies out of {num_seqs_begin} original antibodies\")\n","    #print(f\"Columns: {list(df.columns)}\")\n","    print(f\"\\nClinical trial status distribution:\")\n","    print(df['Highest_Clin_Trial'].value_counts())\n","    print(\"\\n\")\n","\n","    paired_df = pd.read_csv(random_paired_file_path)\n","    # remove duplicates\n","    num_seqs_begin = paired_df.shape[0]\n","    paired_df = paired_df.drop_duplicates(subset=['HeavySequence','LightSequence'])\n","    num_seqs_end = paired_df.shape[0]\n","    # Display basic information about the dataset\n","    print(f\"Loaded {len(paired_df)} antibodies from {random_paired_file_path}\")\n","    print(f\"Removed {num_seqs_begin - num_seqs_end} antibodies out of {num_seqs_begin} original antibodies\")\n","    #print(f\"Columns: {list(paired_df.columns)}\")\n","    print(f\"\\nClinical trial status distribution:\")\n","    print(paired_df['Highest_Clin_Trial'].value_counts())\n","    print(\"\\n\")\n","\n","    df = pd.concat([df,paired_df], ignore_index=True)\n","    # Display basic information about the dataset\n","    print(f\"Combined {len(df)} antibodies\")\n","    #print(f\"Columns: {list(df.columns)}\")\n","    print(f\"\\nClinical trial status distribution:\")\n","    print(df['Highest_Clin_Trial'].value_counts())\n","\n","    # # Separate training antibodies from other antibodies\n","    # approved_mask = ~(df['Highest_Clin_Trial'].str.contains('Hit_Antibody', case=False, na=False))\n","    # #approved_mask = df['Sweet-Jones_Martin_dataset'].str.contains('Y', case=False, na=False)\n","\n","    # df['Status'] = np.where(approved_mask, 'Approved', 'Other')\n","    df['Status'] = df['Highest_Clin_Trial']\n","    print(f\"\\nStatus distribution:\")\n","    print(df['Status'].value_counts())\n","\n","    # Remove any entries with missing sequences\n","    initial_count = len(df)\n","    df = df.dropna(subset=['HeavySequence', 'LightSequence'])\n","    final_count = len(df)\n","\n","    if initial_count != final_count:\n","        print(f\"Removed {initial_count - final_count} entries with missing sequences\")\n","\n","    return df\n","\n","def load_and_align_antibodies(csv_file_path,random_paired_file_path, scheme='martin'):\n","    \"\"\"\n","    Complete pipeline for loading and aligning antibody sequences.\n","\n","    Args:\n","        csv_file_path (str): Path to CSV file\n","        scheme (str): Numbering scheme for alignment\n","\n","    Returns:\n","        tuple: (aligned_dataframe, aligner_object)\n","    \"\"\"\n","\n","    print(\"=\"*60)\n","    print(\"ANTIBODY DATA LOADING AND ALIGNMENT\")\n","    print(\"=\"*60)\n","\n","    # Load data\n","    df = load_antibody_data(csv_file_path, random_paired_file_path)\n","\n","    df.reset_index(inplace=True, drop=True)\n","\n","    # Align sequences\n","    aligner = AntibodyAligner(scheme=scheme)\n","    df_aligned = aligner.align_antibody_sequences(df)\n","\n","    # Print alignment info\n","    print(f\"\\nAlignment Information:\")\n","    info = aligner.get_alignment_info()\n","    for key, value in info.items():\n","        print(f\"  {key}: {value}\")\n","\n","    return df_aligned, aligner\n","\n","df_aligned, aligner = load_and_align_antibodies(therapeutic_antibodies_csv, random_paired_antibodies_csv, scheme=\"martin\")"]},{"cell_type":"code","execution_count":null,"id":"07cd11b3","metadata":{"id":"07cd11b3"},"outputs":[],"source":["heavy_out_positions = [str(x[0])+x[1].strip() for x in aligner.heavy_positions]\n","\n","print(\"The following positions were output by ANARCI but unavailable in the defined numbering scheme. Antibodies with these positions will go unused.\")\n","print([x for x in heavy_out_positions if x not in heavy_numbering_scheme])\n","print(\"The following positions were available in the defined numbering scheme but are unused in the current antibody set:\")\n","print([x for x in heavy_numbering_scheme if x not in heavy_out_positions])\n","\n","light_out_positions = [str(x[0])+x[1].strip() for x in aligner.light_positions]\n","\n","print(\"The following positions were output by ANARCI but unavailable in the defined numbering scheme. Antibodies with these positions will go unused.\")\n","print([x for x in light_out_positions if x not in light_numbering_scheme])\n","print(\"The following positions were available in the defined numbering scheme but are unused in the current antibody set:\")\n","print([x for x in light_numbering_scheme if x not in light_out_positions])"]},{"cell_type":"markdown","id":"2114008c","metadata":{"id":"2114008c"},"source":["Let's look at some aligned sequences"]},{"cell_type":"code","execution_count":null,"id":"646beb36","metadata":{"id":"646beb36"},"outputs":[],"source":["df_aligned['LightAligned'].head(5)  # Display first 5 aligned light sequences"]},{"cell_type":"code","execution_count":null,"id":"21df58aa","metadata":{"id":"21df58aa"},"outputs":[],"source":["df_aligned['HeavyAligned'].head(5)  # Display first 5 aligned heavy sequences\n"]},{"cell_type":"markdown","id":"895ec3c8","metadata":{"id":"895ec3c8"},"source":["# 4. ANTIBERTY EMBEDDING EXTRACTION WITH ALIGNED SEQUENCES"]},{"cell_type":"code","execution_count":null,"id":"42b865ab","metadata":{"id":"42b865ab"},"outputs":[],"source":["class AntiBERTyEmbedder:\n","    \"\"\"\n","    Class for extracting embeddings using the official AntiBERTy implementation.\n","    Uses the jeffreyruffolo/AntiBERTy repository for optimized antibody embeddings.\n","    \"\"\"\n","\n","    def __init__(self, batch_size=32, clear_cache=True):\n","        \"\"\"Initialize the AntiBERTy runner with batching support.\n","            Args:\n","                batch_size (int): Number of sequences to process at once\n","                clear_cache (bool): Whether to clear CUDA cache between batches\n","        \"\"\"\n","        if not ANTIBERTY_AVAILABLE:\n","            raise ImportError(\"AntiBERTy is required. Please install with: pip install antiberty\")\n","\n","        self.batch_size = batch_size\n","        self.clear_cache = clear_cache\n","\n","        print(\"Initializing AntiBERTy runner...\")\n","        try:\n","            self.antiberty = AntiBERTyRunner()\n","            print(\"AntiBERTy runner initialized successfully!\")\n","\n","            # Print memory info\n","            if torch.cuda.is_available():\n","                self._print_gpu_memory(\"Initial GPU memory\")\n","\n","        except Exception as e:\n","            print(f\"Error initializing AntiBERTy: {e}\")\n","            raise\n","\n","    def _print_gpu_memory(self, stage=\"\"):\n","        \"\"\"Print current GPU memory usage.\"\"\"\n","        if torch.cuda.is_available():\n","            allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n","            reserved = torch.cuda.memory_reserved() / 1024**3   # GB\n","            max_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB\n","            print(f\"{stage}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved, {max_memory:.2f}GB total\")\n","\n","    def _clear_gpu_cache(self):\n","        \"\"\"Clear GPU cache and run garbage collection.\"\"\"\n","        if torch.cuda.is_available() and self.clear_cache:\n","            torch.cuda.empty_cache()\n","        gc.collect()\n","\n","    def get_sequence_embeddings(self, sequences, batch_size=None, show_progress=True):\n","        \"\"\"\n","        Extract embeddings for a list of protein sequences with batching.\n","\n","        Args:\n","            sequences (list): List of protein sequences (can contain gaps '-')\n","            batch_size (int, optional): Override default batch size\n","            show_progress (bool): Whether to show progress bar\n","\n","        Returns:\n","            np.ndarray: Array of sequence embeddings\n","        \"\"\"\n","\n","        if batch_size is None:\n","            batch_size = self.batch_size\n","\n","        # Clean sequences - replace gaps with '_' (padding character for AntiBERTy)\n","        cleaned_sequences = []\n","        for seq in sequences:\n","            cleaned_seq = seq.replace('-', '_')\n","            cleaned_sequences.append(cleaned_seq)\n","\n","        num_sequences = len(cleaned_sequences)\n","        num_batches = (num_sequences + batch_size - 1) // batch_size\n","\n","        print(f\"Processing {num_sequences} sequences in {num_batches} batches of size {batch_size}\")\n","\n","        all_embeddings = []\n","\n","        # Process in batches\n","        iterator = range(0, num_sequences, batch_size)\n","        if show_progress:\n","            iterator = tqdm(iterator, desc=\"Processing batches\", unit=\"batch\")\n","\n","        for start_idx in iterator:\n","            end_idx = min(start_idx + batch_size, num_sequences)\n","            batch_sequences = cleaned_sequences[start_idx:end_idx]\n","\n","            try:\n","                # Get embeddings for this batch\n","                #batch_embeddings = self.antiberty.embed(batch_sequences)\n","\n","                formatted_sequences = [\" \".join([ \"[PAD]\" if y==\"_\" else y for y in list(x)]) for x in batch_sequences]\n","                inputs = self.antiberty.tokenizer(formatted_sequences, return_tensors=\"pt\")\n","                device = next(self.antiberty.model.parameters()).device\n","                inputs = {k: v.to(device) for k, v in inputs.items()}\n","                with torch.no_grad():\n","                    outputs = self.antiberty.model(**inputs, output_hidden_states=True)\n","\n","                batch_embeddings = outputs['hidden_states'][-1] # get just the last layer\n","\n","                # Handle CUDA tensors properly\n","                if isinstance(batch_embeddings, torch.Tensor):\n","                    batch_embeddings = batch_embeddings.cpu().detach().numpy()\n","                elif isinstance(batch_embeddings, list):\n","                    processed_embeddings = []\n","                    for emb in batch_embeddings:\n","                        if isinstance(emb, torch.Tensor):\n","                            processed_embeddings.append(emb.cpu().detach().numpy())\n","                        else:\n","                            processed_embeddings.append(np.array(emb))\n","                    batch_embeddings = np.array(processed_embeddings)\n","                elif not isinstance(batch_embeddings, np.ndarray):\n","                    batch_embeddings = np.array(batch_embeddings)\n","                all_embeddings.append(batch_embeddings)\n","\n","                # Clear cache after each batch to free memory\n","                self._clear_gpu_cache()\n","\n","                if show_progress and torch.cuda.is_available():\n","                    # Update progress with memory info every 10 batches\n","                    if (start_idx // batch_size) % 10 == 0:\n","                        allocated = torch.cuda.memory_allocated() / 1024**3\n","                        iterator.set_postfix({\"GPU_GB\": f\"{allocated:.1f}\"})\n","\n","            except Exception as e:\n","                print(f\"Error processing batch {start_idx//batch_size + 1}/{num_batches}: {e}\")\n","\n","                # Try with smaller batch size if CUDA out of memory\n","                if \"out of memory\" in str(e).lower() and batch_size > 1:\n","                    print(f\"Retrying batch with smaller size: {batch_size//2}\")\n","                    smaller_batch_embeddings = self.get_sequence_embeddings(\n","                        batch_sequences,\n","                        batch_size=batch_size//2,\n","                        show_progress=False\n","                    )\n","                    all_embeddings.append(smaller_batch_embeddings)\n","                    self._clear_gpu_cache()\n","                else:\n","                    # Return zero embeddings for failed batch\n","                    print(f\"Using zero embeddings for failed batch\")\n","                    failed_embeddings = np.zeros((len(batch_sequences), 512))  # Assuming 512-dim\n","                    all_embeddings.append(failed_embeddings)\n","        # Concatenate all batch results\n","        if all_embeddings:\n","            final_embeddings = np.concatenate(all_embeddings, axis=0)\n","        else:\n","            final_embeddings = np.zeros((num_sequences, 512))  # Fallback\n","\n","        print(f\"Final embeddings shape: {final_embeddings.shape}\")\n","\n","        if torch.cuda.is_available():\n","            self._print_gpu_memory(\"Final GPU memory\")\n","\n","        return final_embeddings\n","\n","    def get_antibody_embeddings(self, df, use_aligned=True, combine_chains=True,\n","                               gap_strategy='replace', batch_size=None):\n","        \"\"\"\n","        Extract embeddings for all antibodies in the dataframe with batching.\n","\n","        Args:\n","            df (pd.DataFrame): DataFrame with antibody sequences\n","            use_aligned (bool): Whether to use aligned sequences\n","            combine_chains (bool): Whether to combine heavy and light chains\n","            gap_strategy (str): How to handle gaps ('replace', 'remove')\n","            batch_size (int, optional): Override default batch size\n","\n","        Returns:\n","            tuple: (embeddings_array, valid_indices)\n","        \"\"\"\n","\n","        if batch_size is None:\n","            batch_size = self.batch_size\n","\n","        embeddings = []\n","        valid_indices = []\n","\n","        # Choose sequence columns\n","        if use_aligned:\n","            heavy_col = 'HeavyAligned'\n","            light_col = 'LightAligned'\n","            print(f\"Using aligned sequences for embedding extraction\")\n","\n","            # Filter to successfully aligned sequences\n","            valid_mask = df['AlignmentSuccess'] & df[heavy_col].notna() & df[light_col].notna()\n","            df_valid = df[valid_mask].reset_index(drop=True)\n","            valid_indices = df[valid_mask].index.tolist()\n","\n","        else:\n","            heavy_col = 'HeavySequence'\n","            light_col = 'LightSequence'\n","            print(f\"Using original sequences for embedding extraction\")\n","            df_valid = df.copy()\n","            valid_indices = list(range(len(df)))\n","\n","        print(f\"Extracting embeddings for {len(df_valid)} antibodies...\")\n","\n","        if combine_chains:\n","            # Combine heavy and light chains\n","            combined_sequences = []\n","            for idx, row in df_valid.iterrows():\n","                # Combine with separator\n","                combined_seq = row[heavy_col] + '[SEP]' + row[light_col]\n","                combined_sequences.append(combined_seq)\n","\n","            print(\"Getting combined chain embeddings with batching...\")\n","            embeddings = self.get_sequence_embeddings(combined_sequences, batch_size=batch_size)\n","            embeddings = embeddings[:,1:-1] # remove the embedding for the start and end tokens\n","\n","        else:\n","            # Get separate embeddings for heavy and light chains\n","            heavy_sequences = df_valid[heavy_col].tolist()\n","            light_sequences = df_valid[light_col].tolist()\n","\n","            print(\"Getting heavy chain embeddings with batching...\")\n","            heavy_embeddings = self.get_sequence_embeddings(heavy_sequences, batch_size=batch_size)\n","\n","            print(\"Getting light chain embeddings with batching...\")\n","            light_embeddings = self.get_sequence_embeddings(light_sequences, batch_size=batch_size)\n","\n","            # Concatenate heavy and light embeddings\n","            embeddings = np.concatenate([heavy_embeddings[:,1:-1], light_embeddings[:,1:-1]], axis=1)\n","\n","        print(f\"Extracted embeddings with shape: {embeddings.shape}\")\n","        print(f\"Valid antibodies: {len(valid_indices)}/{len(df)}\")\n","\n","        return embeddings, valid_indices\n","\n","    def get_embedding_statistics(self, embeddings):\n","        \"\"\"Get basic statistics about the embeddings.\"\"\"\n","\n","        stats = {\n","            'shape': embeddings.shape,\n","            'mean_norm': np.mean(np.linalg.norm(embeddings, axis=1)),\n","            'std_norm': np.std(np.linalg.norm(embeddings, axis=1)),\n","            'mean_values': np.mean(embeddings, axis=0),\n","            'std_values': np.std(embeddings, axis=0),\n","            'embedding_dim': embeddings.shape[1] if len(embeddings.shape) > 1 else 0\n","        }\n","\n","        print(f\"Embedding Statistics:\")\n","        print(f\"  Shape: {stats['shape']}\")\n","        print(f\"  Embedding dimension: {stats['embedding_dim']}\")\n","        print(f\"  Mean L2 norm: {stats['mean_norm']:.3f} ¬± {stats['std_norm']:.3f}\")\n","        print(f\"  Feature mean range: [{np.min(stats['mean_values']):.3f}, {np.max(stats['mean_values']):.3f}]\")\n","\n","        return stats\n","\n","embedder = AntiBERTyEmbedder(batch_size=32, clear_cache=True)\n","\n","use_aligned_sequences = True  # Set to True to use aligned sequences, False for original sequences\n","if use_aligned_sequences:\n","    embeddings, valid_indices = embedder.get_antibody_embeddings(\n","        df_aligned, use_aligned=True, combine_chains=False\n","    )\n","    # Use only successfully aligned antibodies for analysis\n","    df_analysis = df_aligned.iloc[valid_indices].reset_index(drop=True)\n","    print(f\"Using {len(df_analysis)} successfully aligned antibodies for analysis\")\n","else:\n","    embeddings, valid_indices = embedder.get_antibody_embeddings(\n","        df_aligned, use_aligned=False, combine_chains=False\n","    )\n","    df_analysis = df_aligned.copy()"]},{"cell_type":"markdown","id":"b5df2cb4","metadata":{"id":"b5df2cb4"},"source":["Look at the shape of the embeddings. What are each of the dimensions?"]},{"cell_type":"code","execution_count":null,"id":"41d9e56a","metadata":{"id":"41d9e56a"},"outputs":[],"source":["print(embeddings.shape)"]},{"cell_type":"markdown","id":"f0436205","metadata":{"id":"f0436205"},"source":["The first dimension of the embeddings is the number of samples. The second dimension should be the number of residues in the aligned antibodies. Let's check..."]},{"cell_type":"code","execution_count":null,"id":"c89c1b35","metadata":{"id":"c89c1b35"},"outputs":[],"source":["len(df_aligned['HeavyAligned'][0]) + len(df_aligned['LightAligned'][0])"]},{"cell_type":"markdown","id":"7752d231","metadata":{"id":"7752d231"},"source":["Yes, the length of the aligned residues does match. The tokens signalling the beginning and end of each sequence are removed during concatenation in the function `get_antibody_embeddings`.\n","\n","Finally, what is the last dimension, 512? This is a property of the AntiBERTy architecture. Other language models will have different embedding dimensions.\n","\n","Question: what are the embeddings? Let's look...\n","They're just a numpy array of values extracted from the model."]},{"cell_type":"code","execution_count":null,"id":"40c76069","metadata":{"id":"40c76069"},"outputs":[],"source":["print(type(embeddings[0]))\n","print(embeddings[0])"]},{"cell_type":"markdown","id":"1e405356","metadata":{"id":"1e405356"},"source":["# 5. KERNEL PCA TRAINING"]},{"cell_type":"code","execution_count":null,"id":"fcdc0465","metadata":{"id":"fcdc0465"},"outputs":[],"source":["class AntibodyKernelPCA:\n","    \"\"\"\n","    Kernel PCA implementation for antibody developability assessment.\n","    Uses RBF kernel as mentioned in the Sweet-Jones & Martin methodology.\n","    \"\"\"\n","\n","    def __init__(self, n_components=2, kernel='rbf', gamma=None, scale_embeddings=True):\n","        \"\"\"\n","        Initialize Kernel PCA model.\n","\n","        Args:\n","            n_components (int): Number of principal components\n","            kernel (str): Kernel type ('rbf', 'poly', 'sigmoid', 'cosine')\n","            gamma (float): Kernel coefficient for RBF kernel\n","        \"\"\"\n","        self.n_components = n_components\n","        self.kernel = kernel\n","        self.gamma = gamma\n","        self.kpca = None\n","        self.scale_embeddings = scale_embeddings\n","        self.scaler = StandardScaler()\n","\n","    def _reshape_embeddings(self, embeddings):\n","        \"\"\"\n","        Reshape embeddings to 2D by concatenating the last dimensions.\n","\n","        Args:\n","            embeddings (np.ndarray): Input embeddings\n","\n","        Returns:\n","            np.ndarray: Reshaped embeddings (n_samples, concatenated_features)\n","        \"\"\"\n","        print(f\"Input embeddings shape: {embeddings.shape}\")\n","\n","        if len(embeddings.shape) == 2:\n","            # Already correct shape (n_samples, n_features)\n","            return embeddings\n","\n","        elif len(embeddings.shape) == 3:\n","            # 3D array - concatenate embeddings from dimensions 1 and 2\n","            # Shape: (n_samples, dim1, dim2) -> (n_samples, dim1 * dim2)\n","            # Each sample will have embeddings from all dim1 positions concatenated\n","\n","            n_samples, dim1, dim2 = embeddings.shape\n","\n","            # Reshape to concatenate: flatten last two dimensions while preserving order\n","            embeddings_2d = embeddings.reshape(n_samples, dim1 * dim2)\n","\n","            print(f\"Concatenated 3D embeddings from {embeddings.shape} to {embeddings_2d.shape}\")\n","            print(f\"  Each sample now has {dim1} embedding vectors of size {dim2} concatenated\")\n","            return embeddings_2d\n","\n","        elif len(embeddings.shape) == 4:\n","            # 4D array - concatenate embeddings from dimensions 1, 2, and 3\n","            # Shape: (n_samples, dim1, dim2, dim3) -> (n_samples, dim1 * dim2 * dim3)\n","\n","            n_samples, dim1, dim2, dim3 = embeddings.shape\n","\n","            # Reshape to concatenate: flatten last three dimensions while preserving order\n","            embeddings_2d = embeddings.reshape(n_samples, dim1 * dim2 * dim3)\n","\n","            print(f\"Concatenated 4D embeddings from {embeddings.shape} to {embeddings_2d.shape}\")\n","            print(f\"  Each sample now has {dim1}√ó{dim2} embedding vectors of size {dim3} concatenated\")\n","            return embeddings_2d\n","\n","        elif len(embeddings.shape) == 1:\n","            # 1D array - assume single sample\n","            embeddings_2d = embeddings.reshape(1, -1)\n","            print(f\"Reshaped 1D embeddings from {embeddings.shape} to {embeddings_2d.shape}\")\n","            return embeddings_2d\n","\n","        else:\n","            raise ValueError(f\"Unsupported embeddings shape: {embeddings.shape}\")\n","\n","    def _compute_gamma(self, embeddings_scaled):\n","        \"\"\"\n","        Compute gamma value based on the strategy.\n","\n","        Args:\n","            embeddings_scaled (np.ndarray): Scaled embeddings\n","\n","        Returns:\n","            float: Computed gamma value\n","        \"\"\"\n","        n_features = embeddings_scaled.shape[1]\n","\n","        if self.gamma == 'scale':\n","            # Recommended: 1 / (n_features * variance)\n","            variance = np.var(embeddings_scaled)\n","            gamma_val = 1.0 / (n_features * variance) if variance > 0 else 1.0 / n_features\n","            print(f\"  Gamma strategy: 'scale'\")\n","            print(f\"    Computed gamma = 1/(n_features √ó variance) = {gamma_val:.6f}\")\n","\n","        elif self.gamma == 'auto' or self.gamma is None:\n","            # Default sklearn: 1 / n_features (often too small for high-dim embeddings)\n","            gamma_val = 1.0 / n_features\n","            print(f\"  Gamma strategy: 'auto'\")\n","            print(f\"    Computed gamma = 1/n_features = {gamma_val:.6f}\")\n","            print(f\"  ‚ö† WARNING: This may be too small for high-dimensional embeddings!\")\n","\n","        else:\n","            # User-specified value\n","            gamma_val = float(self.gamma)\n","            print(f\"  Gamma strategy: user-specified\")\n","            print(f\"    Gamma value = {gamma_val:.6f}\")\n","\n","        return gamma_val\n","\n","    def fit_transform(self, embeddings):\n","        \"\"\"\n","        Fit kernel PCA and transform embeddings.\n","\n","        Args:\n","            embeddings (np.ndarray): Antibody embeddings\n","\n","        Returns:\n","            np.ndarray: Transformed embeddings in PC space\n","        \"\"\"\n","\n","        print(\"Fitting Kernel PCA...\")\n","\n","        # Reshape embeddings to 2D if needed\n","        embeddings_2d = self._reshape_embeddings(embeddings)\n","\n","        # Check for any remaining issues\n","        if len(embeddings_2d.shape) != 2:\n","            raise ValueError(f\"After reshaping, embeddings still not 2D: {embeddings_2d.shape}\")\n","\n","        # Check for NaN or infinite values\n","        if np.any(np.isnan(embeddings_2d)):\n","            print(\"Warning: NaN values found in embeddings, replacing with zeros\")\n","            embeddings_2d = np.nan_to_num(embeddings_2d)\n","\n","        if np.any(np.isinf(embeddings_2d)):\n","            print(\"Warning: Infinite values found in embeddings, replacing with finite values\")\n","            embeddings_2d = np.nan_to_num(embeddings_2d)\n","\n","        print(f\"Final embeddings shape for PCA: {embeddings_2d.shape}\")\n","        print(f\"Embedding statistics: mean={np.mean(embeddings_2d):.3f}, std={np.std(embeddings_2d):.3f}\")\n","\n","        # Standardize embeddings\n","        if self.scale_embeddings:\n","            try:\n","                embeddings_scaled = self.scaler.fit_transform(embeddings_2d)\n","                print(\"‚úì StandardScaler fit successful\")\n","            except Exception as e:\n","                print(f\"Error in StandardScaler: {e}\")\n","                print(f\"Embeddings shape: {embeddings_2d.shape}\")\n","                print(f\"Embeddings dtype: {embeddings_2d.dtype}\")\n","                raise\n","        else:\n","            embeddings_scaled = embeddings_2d\n","\n","        # Initialize Kernel PCA\n","        gamma = self._compute_gamma(embeddings_scaled)\n","\n","        self.kpca = KernelPCA(\n","            n_components=self.n_components,\n","            kernel=self.kernel,\n","            gamma=gamma,\n","            random_state=42,\n","            n_jobs=-1\n","        )\n","\n","        # Fit and transform\n","        try:\n","            embeddings_transformed = self.kpca.fit_transform(embeddings_scaled)\n","            print(\"‚úì Kernel PCA fit_transform successful\")\n","        except Exception as e:\n","            print(f\"Error in Kernel PCA: {e}\")\n","            print(f\"Scaled embeddings shape: {embeddings_scaled.shape}\")\n","            raise\n","\n","        print(f\"Kernel PCA completed. Output shape: {embeddings_transformed.shape}\")\n","\n","        # Try to get explained variance ratio\n","        explained_var = self.get_explained_variance_ratio()\n","        if explained_var is not None:\n","            print(f\"Explained variance ratio (approximation): {explained_var}\")\n","\n","        return embeddings_transformed\n","\n","    def transform(self, embeddings):\n","        \"\"\"Transform new embeddings using fitted model.\"\"\"\n","        if self.kpca is None:\n","            raise ValueError(\"Model must be fitted before transforming new data\")\n","\n","        # Reshape if needed\n","        embeddings_2d = self._reshape_embeddings(embeddings)\n","\n","        # Scale using fitted scaler\n","        embeddings_scaled = self.scaler.transform(embeddings_2d)\n","\n","        return self.kpca.transform(embeddings_scaled)\n","\n","    def get_explained_variance_ratio(self):\n","        \"\"\"\n","        Approximate explained variance ratio for kernel PCA.\n","        Note: This is an approximation as exact calculation is complex for kernel PCA.\n","        \"\"\"\n","        if self.kpca is None:\n","            return None\n","\n","        try:\n","            # Get eigenvalues from the kernel matrix\n","            eigenvalues = self.kpca.eigenvalues_\n","            if eigenvalues is not None and len(eigenvalues) > 0:\n","                # Take only positive eigenvalues\n","                positive_eigenvalues = eigenvalues[eigenvalues > 0]\n","                if len(positive_eigenvalues) > 0:\n","                    total_variance = np.sum(positive_eigenvalues)\n","                    explained_variance_ratio = positive_eigenvalues / total_variance\n","                    return explained_variance_ratio[:self.n_components]\n","            return None\n","        except Exception as e:\n","            print(f\"Could not compute explained variance ratio: {e}\")\n","            return None\n","\n","if not 'kpca_gamma_dict' in locals():\n","    kpca_gamma_dict = {}\n","for gamma in [ 100, 500]: # add additional values to tune the PCA\n","    if gamma in kpca_gamma_dict.keys(): continue\n","    print(gamma)\n","    kpca_model = AntibodyKernelPCA(gamma=gamma, scale_embeddings=False)\n","    pc_embeddings = kpca_model.fit_transform(embeddings)\n","    kpca_gamma_dict[gamma] = (kpca_model, pc_embeddings)"]},{"cell_type":"markdown","id":"6bca59d0","metadata":{"id":"6bca59d0"},"source":["# 6. VISUALIZATION OF KERNEL PCA RESULTS"]},{"cell_type":"code","execution_count":null,"id":"e54cf8f4","metadata":{"id":"e54cf8f4"},"outputs":[],"source":["def plot_kernel_pca_results(pc_embeddings, labels, status_col='Status',\n","                           title=\"Antibody Developability Landscape\",\n","                           figsize=(12, 8)):\n","    \"\"\"\n","    Plot kernel PCA results with approved vs other antibodies in different colors.\n","\n","    Args:\n","        pc_embeddings (np.ndarray): PC coordinates\n","        labels (pd.Series or array): Status labels (Approved/Other)\n","        status_col (str): Column name for status\n","        title (str): Plot title\n","        figsize (tuple): Figure size\n","    \"\"\"\n","\n","    plt.figure(figsize=figsize)\n","\n","    # Create color mapping\n","    unique_labels = np.unique(labels)\n","    colors = ['#2E86AB','#C73E1D', '#A23B72', '#F18F01', \"#28C71D\"]\n","    color_map = {label: colors[i % len(colors)] for i, label in enumerate(unique_labels)}\n","\n","    # Plot each group\n","    for label in sorted(unique_labels):\n","        mask = labels == label\n","        if label==\"Approved\":\n","            z = 100\n","        else:\n","            z=1\n","        plt.scatter(\n","            pc_embeddings[mask, 0],\n","            pc_embeddings[mask, 1],\n","            c=color_map[label],\n","            label=f'{label} (n={np.sum(mask)})',\n","            alpha=0.7,\n","            s=10,\n","            edgecolors='white',\n","            linewidth=0.5,\n","            zorder=z\n","        )\n","\n","    plt.xlabel('PC1', fontsize=12)\n","    plt.ylabel('PC2', fontsize=12)\n","    plt.title(title, fontsize=14, fontweight='bold')\n","    plt.xlim(-0.1,0.1)\n","    plt.ylim(-0.1,0.1)\n","    plt.legend(fontsize=10)\n","    plt.grid(True, alpha=0.3)\n","\n","    # Add statistics\n","    approved_mask = labels == 'Approved'\n","    other_mask = labels == 'Other'\n","\n","    if np.any(approved_mask) and np.any(other_mask):\n","        # Calculate centroids\n","        approved_centroid = np.mean(pc_embeddings[approved_mask], axis=0)\n","        other_centroid = np.mean(pc_embeddings[other_mask], axis=0)\n","\n","        # Plot centroids\n","        plt.scatter(*approved_centroid, c='red', s=200, marker='x', linewidth=3, label='Approved Centroid')\n","        plt.scatter(*other_centroid, c='blue', s=200, marker='x', linewidth=3, label='Other Centroid')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Print summary statistics\n","    print(\"\\nSummary Statistics:\")\n","    print(f\"Total antibodies: {len(pc_embeddings)}\")\n","    for label in unique_labels:\n","        mask = labels == label\n","        count = np.sum(mask)\n","        pc1_mean = np.mean(pc_embeddings[mask, 0])\n","        pc2_mean = np.mean(pc_embeddings[mask, 1])\n","        pc1_std = np.std(pc_embeddings[mask, 0])\n","        pc2_std = np.std(pc_embeddings[mask, 1])\n","\n","        print(f\"\\n{label} antibodies (n={count}):\")\n","        print(f\"  PC1: {pc1_mean:.3f} ¬± {pc1_std:.3f}\")\n","        print(f\"  PC2: {pc2_mean:.3f} ¬± {pc2_std:.3f}\")\n","\n","print(kpca_gamma_dict.keys())\n","kpca_model, pc_embeddings = kpca_gamma_dict[200]\n","plot_kernel_pca_results(pc_embeddings, df_analysis['Status'])"]},{"cell_type":"code","execution_count":null,"id":"430eafa8","metadata":{"id":"430eafa8"},"outputs":[],"source":["kpca_gamma_dict.keys()"]},{"cell_type":"markdown","id":"a651268d","metadata":{"id":"a651268d"},"source":["# 7. ELLIPSE SELECTION STRATEGY\n"]},{"cell_type":"code","execution_count":null,"id":"90d57398","metadata":{"id":"90d57398"},"outputs":[],"source":["\n","class EllipseSelector:\n","    \"\"\"\n","    Ellipse-based selection strategy for identifying developable antibodies.\n","    Based on Z-scores for PC1 and PC2 as described in Sweet-Jones & Martin.\n","    \"\"\"\n","\n","    def __init__(self, z_threshold=2.0):\n","        \"\"\"\n","        Initialize ellipse selector.\n","\n","        Args:\n","            z_threshold (float): Z-score threshold for ellipse boundary (default: 2.0)\n","        \"\"\"\n","        self.z_threshold = z_threshold\n","        self.approved_centroid = None\n","        self.approved_std = None\n","        self.fitted = False\n","\n","    def fit(self, pc_embeddings, labels):\n","        \"\"\"\n","        Fit ellipse parameters based on approved antibodies.\n","\n","        Args:\n","            pc_embeddings (np.ndarray): PC coordinates\n","            labels (array-like): Status labels\n","        \"\"\"\n","\n","        # Get approved antibodies\n","        approved_mask = labels == 'Approved'\n","\n","        if not np.any(approved_mask):\n","            raise ValueError(\"No approved antibodies found in the dataset\")\n","\n","        approved_coords = pc_embeddings[approved_mask]\n","\n","        # Calculate centroid and standard deviations\n","        self.approved_centroid = np.mean(approved_coords, axis=0)\n","        self.approved_std = np.std(approved_coords, axis=0)\n","\n","        # Handle case where std is very small\n","        self.approved_std = np.maximum(self.approved_std, 1e-6)\n","\n","        self.fitted = True\n","\n","        print(f\"Ellipse fitted to {np.sum(approved_mask)} approved antibodies\")\n","        print(f\"Centroid: PC1={self.approved_centroid[0]:.3f}, PC2={self.approved_centroid[1]:.3f}\")\n","        print(f\"Std Dev: PC1={self.approved_std[0]:.3f}, PC2={self.approved_std[1]:.3f}\")\n","        print(f\"Z-score threshold: {self.z_threshold}\")\n","\n","    def calculate_z_scores(self, pc_embeddings):\n","        \"\"\"\n","        Calculate Z-scores for PC coordinates relative to approved centroid.\n","\n","        Args:\n","            pc_embeddings (np.ndarray): PC coordinates\n","\n","        Returns:\n","            np.ndarray: Z-scores for each coordinate\n","        \"\"\"\n","\n","        if not self.fitted:\n","            raise ValueError(\"Selector must be fitted before calculating Z-scores\")\n","\n","        z_scores = np.abs(pc_embeddings - self.approved_centroid) / self.approved_std\n","        return z_scores\n","\n","    def is_within_ellipse(self, pc_embeddings):\n","        \"\"\"\n","        Determine which antibodies are within the ellipse.\n","\n","        Args:\n","            pc_embeddings (np.ndarray): PC coordinates\n","\n","        Returns:\n","            np.ndarray: Boolean mask indicating which antibodies are within ellipse\n","        \"\"\"\n","\n","        z_scores = self.calculate_z_scores(pc_embeddings)\n","\n","        # Ellipse equation: (z1/threshold)¬≤ + (z2/threshold)¬≤ <= 1\n","        ellipse_distance = np.sum((z_scores / self.z_threshold) ** 2, axis=1)\n","        within_ellipse = ellipse_distance <= 1.0\n","\n","        return within_ellipse\n","\n","    def plot_ellipse(self, pc_embeddings, labels, figsize=(12, 8)):\n","        \"\"\"\n","        Plot the ellipse selection boundary along with the data points.\n","        \"\"\"\n","\n","        if not self.fitted:\n","            raise ValueError(\"Selector must be fitted before plotting\")\n","\n","        plt.figure(figsize=figsize)\n","\n","        # Plot data points\n","        unique_labels = np.unique(labels)\n","        colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n","        color_map = {label: colors[i % len(colors)] for i, label in enumerate(unique_labels)}\n","\n","        for label in unique_labels:\n","            zorder=1\n","            if label==\"Approved\":\n","                zorder=8\n","            mask = labels == label\n","            plt.scatter(\n","                pc_embeddings[mask, 0],\n","                pc_embeddings[mask, 1],\n","                c=color_map[label],\n","                label=f'{label} (n={np.sum(mask)})',\n","                alpha=0.5,\n","                s=20,\n","                edgecolors='white',\n","                linewidth=0.5,\n","                zorder=zorder\n","            )\n","\n","        # Plot ellipse\n","        theta = np.linspace(0, 2*np.pi, 100)\n","        ellipse_x = self.approved_centroid[0] + self.z_threshold * self.approved_std[0] * np.cos(theta)\n","        ellipse_y = self.approved_centroid[1] + self.z_threshold * self.approved_std[1] * np.sin(theta)\n","\n","        plt.plot(ellipse_x, ellipse_y, 'r--', linewidth=2, zorder=1000,\n","                label=f'Selection Ellipse (Z={self.z_threshold})')\n","\n","        # Plot centroid\n","        plt.scatter(*self.approved_centroid, c='red', s=200, marker='x',\n","                   linewidth=3, label='Approved Centroid')\n","\n","        # Highlight selected antibodies\n","        # within_ellipse = self.is_within_ellipse(pc_embeddings)\n","        # selected_coords = pc_embeddings[within_ellipse]\n","        # plt.scatter(selected_coords[:, 0], selected_coords[:, 1],\n","        #            facecolors='none', edgecolors='red', linewidth=2, s=120,\n","        #            label=f'Selected (n={np.sum(within_ellipse)})')\n","\n","        plt.xlim(-0.1, 0.1)\n","        plt.ylim(-0.1, 0.1)\n","        plt.xlabel('PC1', fontsize=12)\n","        plt.ylabel('PC2', fontsize=12)\n","        plt.title('Antibody Selection using Ellipse Strategy', fontsize=14, fontweight='bold')\n","        plt.legend(fontsize=10)\n","        plt.grid(True, alpha=0.3)\n","        plt.tight_layout()\n","        plt.show()"]},{"cell_type":"markdown","id":"c30945cb","metadata":{"id":"c30945cb"},"source":["# 8. ANTIBODY SELECTION FUNCTION"]},{"cell_type":"code","execution_count":null,"id":"ff359af4","metadata":{"id":"ff359af4"},"outputs":[],"source":["def select_developable_antibodies(df, pc_embeddings, z_threshold=2.0,\n","                                 return_details=True):\n","    \"\"\"\n","    Complete pipeline for selecting developable antibodies using ellipse strategy.\n","\n","    Args:\n","        df (pd.DataFrame): Antibody dataframe\n","        pc_embeddings (np.ndarray): PC coordinates\n","        z_threshold (float): Z-score threshold for ellipse\n","        return_details (bool): Whether to return detailed analysis\n","\n","    Returns:\n","        dict: Selection results and analysis\n","    \"\"\"\n","\n","    print(\"=\"*60)\n","    print(\"ANTIBODY DEVELOPABILITY SELECTION PIPELINE\")\n","    print(\"=\"*60)\n","\n","    # Initialize selector\n","    selector = EllipseSelector(z_threshold=z_threshold)\n","\n","    # Fit ellipse to approved antibodies\n","    selector.fit(pc_embeddings, df['Status'])\n","\n","    # Select antibodies within ellipse\n","    within_ellipse = selector.is_within_ellipse(pc_embeddings)\n","    selected_antibodies = df[within_ellipse].copy()\n","\n","    # Calculate Z-scores for all antibodies\n","    z_scores = selector.calculate_z_scores(pc_embeddings)\n","    df_with_scores = df.copy()\n","    df_with_scores['PC1_ZScore'] = z_scores[:, 0]\n","    df_with_scores['PC2_ZScore'] = z_scores[:, 1]\n","    df_with_scores['Within_Ellipse'] = within_ellipse\n","    df_with_scores['Ellipse_Distance'] = np.sum((z_scores / z_threshold) ** 2, axis=1)\n","\n","    # Analysis\n","    total_antibodies = len(df)\n","    selected_count = np.sum(within_ellipse)\n","    approved_count = np.sum(df['Status'] == 'Approved')\n","    approved_selected = np.sum((df['Status'] == 'Approved') & within_ellipse)\n","    other_selected = np.sum((df['Status'] == 'Hit_Antibody') & within_ellipse)\n","\n","    # Calculate enrichment\n","    if selected_count > 0:\n","        approved_enrichment = (approved_selected / selected_count) / (approved_count / total_antibodies)\n","    else:\n","        approved_enrichment = 0\n","\n","    print(f\"\\nSELECTION RESULTS:\")\n","    print(f\"Total antibodies: {total_antibodies}\")\n","    print(f\"Selected antibodies: {selected_count} ({100*selected_count/total_antibodies:.1f}%)\")\n","    print(f\"Approved antibodies selected: {approved_selected}/{approved_count} ({100*approved_selected/approved_count:.1f}%)\")\n","    print(f\"Other antibodies selected: {other_selected}/{total_antibodies-approved_count} ({100*other_selected/(total_antibodies-approved_count):.1f}%)\")\n","    print(f\"Approved enrichment factor: {approved_enrichment:.2f}\")\n","\n","    # Plot results\n","    selector.plot_ellipse(pc_embeddings, df['Status'])\n","\n","    results = {\n","        'selector': selector,\n","        'selected_antibodies': selected_antibodies,\n","        'df_with_scores': df_with_scores,\n","        'within_ellipse_mask': within_ellipse,\n","        'total_count': total_antibodies,\n","        'selected_count': selected_count,\n","        'approved_selected': approved_selected,\n","        'other_selected': other_selected,\n","        'enrichment_factor': approved_enrichment,\n","        'z_threshold': z_threshold\n","    }\n","\n","    if return_details:\n","        return results\n","    else:\n","        return selected_antibodies\n","\n","def rank_antibodies_by_developability(df, pc_embeddings, z_threshold=2.0):\n","    \"\"\"\n","    Rank all antibodies by their developability score (inverse ellipse distance).\n","\n","    Args:\n","        df (pd.DataFrame): Antibody dataframe\n","        pc_embeddings (np.ndarray): PC coordinates\n","        z_threshold (float): Z-score threshold\n","\n","    Returns:\n","        pd.DataFrame: Ranked antibodies with developability scores\n","    \"\"\"\n","\n","    # Fit selector\n","    selector = EllipseSelector(z_threshold=z_threshold)\n","    selector.fit(pc_embeddings, df['Status'])\n","\n","    # Calculate scores\n","    z_scores = selector.calculate_z_scores(pc_embeddings)\n","    ellipse_distances = np.sum((z_scores / z_threshold) ** 2, axis=1)\n","\n","    # Developability score (higher is better)\n","    developability_scores = 1.0 / (1.0 + ellipse_distances)\n","\n","    # Create ranked dataframe\n","    df_ranked = df.copy()\n","    df_ranked['PC1_ZScore'] = z_scores[:, 0]\n","    df_ranked['PC2_ZScore'] = z_scores[:, 1]\n","    df_ranked['Ellipse_Distance'] = ellipse_distances\n","    df_ranked['Developability_Score'] = developability_scores\n","    df_ranked['Within_Ellipse'] = ellipse_distances <= 1.0\n","    df_ranked['Rank'] = df_ranked['Developability_Score'].rank(method='dense', ascending=False)\n","\n","    # Sort by developability score\n","    df_ranked = df_ranked.sort_values('Developability_Score', ascending=False)\n","\n","    print(f\"Antibodies ranked by developability score\")\n","    print(f\"Top 10 most developable antibodies:\")\n","    print(df_ranked[['Therapeutic', 'Status', 'Developability_Score', 'Within_Ellipse']].head(10))\n","\n","    return df_ranked\n","\n","kpca_model, pc_embeddings = kpca_gamma_dict[500]\n","z_threshold = 2.0  # Z-score threshold for ellipse selection\n","results = select_developable_antibodies(df_analysis, pc_embeddings, z_threshold)\n","df_ranked = rank_antibodies_by_developability(df_analysis, pc_embeddings, z_threshold=z_threshold)"]},{"cell_type":"markdown","id":"bcb43f56","metadata":{"id":"bcb43f56"},"source":["Compare the results of this KernelPCA with the one in Figure 1 of the manuscript. Why could these be different?\n","- Different implementation details\n","- Different antibodies in the Approved set and the Hit and Unapproved sets.\n","- Fewer outliers in the random selection of Hit antibodies\n","- The original authors use other methods to filter the hit antibodies. One would think this would remove outliers, but we don't see that here.\n","\n","\n","Let's explore the ranked antibodies.\n","\n","- How is the Developability Score calculated?\n","- If you were an antibody engineer, what other criteria would you like to plot?"]},{"cell_type":"code","execution_count":null,"id":"bf205188","metadata":{"id":"bf205188"},"outputs":[],"source":["df_ranked.sort_values('Developability_Score', ascending=False)[['Therapeutic','Developability_Score','PC1_ZScore', 'PC2_ZScore','Rank']].head(10)"]},{"cell_type":"markdown","id":"3cb317ba","metadata":{"id":"3cb317ba"},"source":["# 9. RUN FULL PIPELINE DEMONSTRATION\n","\n","This section is not necessary, but it's nice not to have to hit Go for each cell (though you do need to define the functions.)"]},{"cell_type":"code","execution_count":null,"id":"8c5697d9","metadata":{"id":"8c5697d9"},"outputs":[],"source":["def run_complete_pipeline(therapeutic_csv_file_path, paired_csv_file_path, z_threshold=2.0, use_aligned_sequences=True,\n","                         numbering_scheme='chothia'):\n","    \"\"\"\n","    Run the complete antibody developability assessment pipeline with sequence alignment.\n","\n","    Args:\n","        csv_file_path (str): Path to CSV file with antibody data\n","        z_threshold (float): Z-score threshold for ellipse selection\n","        use_aligned_sequences (bool): Whether to use ANARCI-aligned sequences\n","        numbering_scheme (str): Numbering scheme for alignment ('chothia', 'kabat', 'imgt')\n","\n","    Returns:\n","        dict: Complete pipeline results\n","    \"\"\"\n","\n","    print(\"=\"*80)\n","    print(\"ANTIBODY DEVELOPABILITY TRIAGING PIPELINE WITH SEQUENCE ALIGNMENT\")\n","    print(\"Based on Sweet-Jones & Martin methodology with Chothia numbering\")\n","    print(\"=\"*80)\n","\n","    # Step 1: Load and align data\n","    print(\"\\n1. Loading and aligning antibody sequences...\")\n","    df_aligned, aligner = load_and_align_antibodies(therapeutic_csv_file_path, paired_csv_file_path, scheme=numbering_scheme)\n","\n","    # Step 2: Extract embeddings\n","    print(\"\\n2. Extracting AntiBERTy embeddings...\")\n","    embedder = AntiBERTyEmbedder()\n","\n","    if use_aligned_sequences:\n","        embeddings, valid_indices = embedder.get_antibody_embeddings(\n","            df_aligned, use_aligned=True, combine_chains=True\n","        )\n","        # Use only successfully aligned antibodies for analysis\n","        df_analysis = df_aligned.iloc[valid_indices].reset_index(drop=True)\n","        print(f\"Using {len(df_analysis)} successfully aligned antibodies for analysis\")\n","    else:\n","        embeddings, valid_indices = embedder.get_antibody_embeddings(\n","            df_aligned, use_aligned=False, combine_chains=True\n","        )\n","        df_analysis = df_aligned.copy()\n","\n","    # Step 3: Apply Kernel PCA\n","    print(\"\\n3. Applying Kernel PCA...\")\n","    kpca_model = AntibodyKernelPCA(n_components=2)\n","    pc_embeddings = kpca_model.fit_transform(embeddings)\n","\n","    # Step 4: Visualize results\n","    print(\"\\n4. Visualizing Kernel PCA results...\")\n","    plot_kernel_pca_results(pc_embeddings, df_analysis['Status'])\n","\n","    # Step 5: Select developable antibodies\n","    print(\"\\n5. Selecting developable antibodies...\")\n","    results = select_developable_antibodies(df_analysis, pc_embeddings, z_threshold)\n","\n","    # Step 6: Rank all antibodies\n","    print(\"\\n6. Ranking antibodies by developability...\")\n","    df_ranked = rank_antibodies_by_developability(df_analysis, pc_embeddings, z_threshold)\n","\n","    # Step 7: Alignment quality analysis\n","    #print(\"\\n7. Analyzing alignment quality...\")\n","    #alignment_stats = analyze_alignment_quality(df_aligned, aligner)\n","\n","    # Compile final results\n","    pipeline_results = {\n","        'original_df': df_aligned,\n","        'analysis_df': df_analysis,\n","        'aligner': aligner,\n","        'alignment_stats': alignment_stats,\n","        'embeddings': embeddings,\n","        'valid_indices': valid_indices,\n","        'pc_embeddings': pc_embeddings,\n","        'kpca_model': kpca_model,\n","        'embedder': embedder,\n","        'selection_results': results,\n","        'ranked_antibodies': df_ranked,\n","        'use_aligned_sequences': use_aligned_sequences,\n","        'numbering_scheme': numbering_scheme\n","    }\n","\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n","    print(\"=\"*80)\n","\n","    return pipeline_results\n","\n","def analyze_alignment_quality(df_aligned, aligner):\n","    \"\"\"\n","    Analyze the quality of sequence alignments.\n","\n","    Args:\n","        df_aligned (pd.DataFrame): DataFrame with alignment results\n","        aligner (AntibodyAligner): Aligner object with position information\n","\n","    Returns:\n","        dict: Alignment quality statistics\n","    \"\"\"\n","\n","    successful_alignments = df_aligned['AlignmentSuccess'].sum()\n","    total_antibodies = len(df_aligned)\n","\n","    # Calculate gap statistics for aligned sequences\n","    if successful_alignments > 0:\n","        aligned_df = df_aligned[df_aligned['AlignmentSuccess']]\n","\n","        # Heavy chain gap analysis\n","        heavy_gap_counts = []\n","        heavy_lengths = []\n","        for seq in aligned_df['HeavyAligned']:\n","            if seq:\n","                gap_count = seq.count('-')\n","                length = len(seq)\n","                heavy_gap_counts.append(gap_count)\n","                heavy_lengths.append(length)\n","\n","        # Light chain gap analysis\n","        light_gap_counts = []\n","        light_lengths = []\n","        for seq in aligned_df['LightAligned']:\n","            if seq:\n","                gap_count = seq.count('-')\n","                length = len(seq)\n","                light_gap_counts.append(gap_count)\n","                light_lengths.append(length)\n","\n","        stats = {\n","            'total_antibodies': total_antibodies,\n","            'successful_alignments': successful_alignments,\n","            'alignment_success_rate': successful_alignments / total_antibodies,\n","            'heavy_chain_stats': {\n","                'avg_length': np.mean(heavy_lengths) if heavy_lengths else 0,\n","                'avg_gaps': np.mean(heavy_gap_counts) if heavy_gap_counts else 0,\n","                'max_gaps': max(heavy_gap_counts) if heavy_gap_counts else 0,\n","                'alignment_positions': len(aligner.heavy_positions) if aligner.heavy_positions else 0\n","            },\n","            'light_chain_stats': {\n","                'avg_length': np.mean(light_lengths) if light_lengths else 0,\n","                'avg_gaps': np.mean(light_gap_counts) if light_gap_counts else 0,\n","                'max_gaps': max(light_gap_counts) if light_gap_counts else 0,\n","                'alignment_positions': len(aligner.light_positions) if aligner.light_positions else 0\n","            }\n","        }\n","\n","        print(f\"Alignment Quality Analysis:\")\n","        print(f\"  Success rate: {stats['alignment_success_rate']:.1%}\")\n","        print(f\"  Heavy chain aligned length: {stats['heavy_chain_stats']['alignment_positions']} positions\")\n","        print(f\"  Light chain aligned length: {stats['light_chain_stats']['alignment_positions']} positions\")\n","        print(f\"  Average gaps - Heavy: {stats['heavy_chain_stats']['avg_gaps']:.1f}, Light: {stats['light_chain_stats']['avg_gaps']:.1f}\")\n","\n","    else:\n","        stats = {\n","            'total_antibodies': total_antibodies,\n","            'successful_alignments': 0,\n","            'alignment_success_rate': 0,\n","            'heavy_chain_stats': {},\n","            'light_chain_stats': {}\n","        }\n","\n","    return stats\n","\n","def plot_alignment_overview(df_aligned, aligner, sample_size=5):\n","    \"\"\"\n","    Visualize alignment results for a sample of antibodies.\n","\n","    Args:\n","        df_aligned (pd.DataFrame): DataFrame with alignment results\n","        aligner (AntibodyAligner): Aligner object\n","        sample_size (int): Number of antibodies to display\n","    \"\"\"\n","\n","    # Sample successful alignments\n","    successful_df = df_aligned[df_aligned['AlignmentSuccess']]\n","    if len(successful_df) == 0:\n","        print(\"No successful alignments to display\")\n","        return\n","\n","    sample_df = successful_df.sample(min(sample_size, len(successful_df)))\n","\n","    fig, axes = plt.subplots(len(sample_df), 2, figsize=(16, 3*len(sample_df)))\n","    if len(sample_df) == 1:\n","        axes = axes.reshape(1, -1)\n","\n","    for i, (idx, row) in enumerate(sample_df.iterrows()):\n","        # Heavy chain\n","        heavy_aligned = row['HeavyAligned']\n","        heavy_original = row['HeavySequence']\n","\n","        # Light chain\n","        light_aligned = row['LightAligned']\n","        light_original = row['LightSequence']\n","\n","        # Plot heavy chain\n","        ax_heavy = axes[i, 0]\n","        ax_heavy.text(0.02, 0.7, f\"Original ({len(heavy_original)} aa):\", fontsize=10, weight='bold')\n","        ax_heavy.text(0.02, 0.5, heavy_original[:80] + ('...' if len(heavy_original) > 80 else ''),\n","                     fontsize=8, family='monospace')\n","        ax_heavy.text(0.02, 0.3, f\"Aligned ({len(heavy_aligned)} pos):\", fontsize=10, weight='bold')\n","        ax_heavy.text(0.02, 0.1, heavy_aligned[:80] + ('...' if len(heavy_aligned) > 80 else ''),\n","                     fontsize=8, family='monospace')\n","        ax_heavy.set_title(f\"{row['Therapeutic']} - Heavy Chain\", fontsize=12)\n","        ax_heavy.set_xlim(0, 1)\n","        ax_heavy.set_ylim(0, 1)\n","        ax_heavy.axis('off')\n","\n","        # Plot light chain\n","        ax_light = axes[i, 1]\n","        ax_light.text(0.02, 0.7, f\"Original ({len(light_original)} aa):\", fontsize=10, weight='bold')\n","        ax_light.text(0.02, 0.5, light_original[:80] + ('...' if len(light_original) > 80 else ''),\n","                     fontsize=8, family='monospace')\n","        ax_light.text(0.02, 0.3, f\"Aligned ({len(light_aligned)} pos):\", fontsize=10, weight='bold')\n","        ax_light.text(0.02, 0.1, light_aligned[:80] + ('...' if len(light_aligned) > 80 else ''),\n","                     fontsize=8, family='monospace')\n","        ax_light.set_title(f\"{row['Therapeutic']} - Light Chain\", fontsize=12)\n","        ax_light.set_xlim(0, 1)\n","        ax_light.set_ylim(0, 1)\n","        ax_light.axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()\n"]},{"cell_type":"markdown","id":"a4ffc5a1","metadata":{"id":"a4ffc5a1"},"source":["# Vibe Coding Extras"]},{"cell_type":"code","execution_count":null,"id":"30665d7e","metadata":{"id":"30665d7e"},"outputs":[],"source":["# =============================================================================\n","# EXAMPLE EXECUTION\n","# =============================================================================\n","\n","\"\"\"\n","To run the complete pipeline, uncomment the following lines and provide your CSV file:\n","\n","# Example usage:\n","results = run_complete_pipeline('your_antibody_data.csv','paired_antibodies.csv', z_threshold=2.0)\n","\n","# Access specific results:\n","selected_antibodies = results['selection_results']['selected_antibodies']\n","ranked_antibodies = results['ranked_antibodies']\n","\n","# Or run individual steps:\n","df = load_antibody_data('your_antibody_data.csv')\n","embedder = AntiBERTyEmbedder()\n","embeddings = embedder.get_antibody_embeddings(df)\n","kpca_model = AntibodyKernelPCA(n_components=2)\n","pc_embeddings = kpca_model.fit_transform(embeddings)\n","plot_kernel_pca_results(pc_embeddings, df['Status'])\n","selection_results = select_developable_antibodies(df, pc_embeddings, z_threshold=2.0)\n","\"\"\"\n","\n","print(\"\\nAntibody Developability Pipeline Setup Complete!\")\n","print(\"Load your CSV file and run the pipeline using:\")\n","print(\"results = run_complete_pipeline('your_file.csv')\")"]},{"cell_type":"code","execution_count":null,"id":"33fc7e4f","metadata":{"id":"33fc7e4f"},"outputs":[],"source":["# Memory management utilities\n","def suggest_batch_size_for_gpu():\n","    \"\"\"Suggest appropriate batch size based on GPU memory.\"\"\"\n","\n","    if not torch.cuda.is_available():\n","        return 32  # Default for CPU\n","\n","    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB\n","\n","    # Conservative estimates based on GPU memory\n","    if total_memory >= 24:      # RTX 4090, A100, etc.\n","        return 64\n","    elif total_memory >= 16:    # RTX 4070Ti Super, etc.\n","        return 32\n","    elif total_memory >= 12:    # RTX 4070, etc.\n","        return 16\n","    elif total_memory >= 8:     # RTX 4060, etc.\n","        return 8\n","    else:                       # Lower memory GPUs\n","        return 4\n","\n","print(f\"Suggested batch size for your GPU: {suggest_batch_size_for_gpu()}\")"]},{"cell_type":"code","execution_count":null,"id":"e0dd0e3d","metadata":{"id":"e0dd0e3d"},"outputs":[],"source":["# Additional utility function to debug embedding shapes\n","def debug_embedding_shapes(embeddings):\n","    \"\"\"\n","    Debug function to understand embedding shapes and content.\n","\n","    Args:\n","        embeddings: The embeddings array to debug\n","    \"\"\"\n","    print(\"=\"*50)\n","    print(\"EMBEDDING SHAPE DEBUG\")\n","    print(\"=\"*50)\n","\n","    print(f\"Type: {type(embeddings)}\")\n","    print(f\"Shape: {embeddings.shape}\")\n","    print(f\"Dtype: {embeddings.dtype}\")\n","    print(f\"Number of dimensions: {len(embeddings.shape)}\")\n","\n","    if len(embeddings.shape) >= 1:\n","        print(f\"Dimension 0 (samples): {embeddings.shape[0]}\")\n","    if len(embeddings.shape) >= 2:\n","        print(f\"Dimension 1: {embeddings.shape[1]}\")\n","    if len(embeddings.shape) >= 3:\n","        print(f\"Dimension 2: {embeddings.shape[2]}\")\n","    if len(embeddings.shape) >= 4:\n","        print(f\"Dimension 3: {embeddings.shape[3]}\")\n","\n","    # Show some statistics\n","    print(f\"\\nStatistics:\")\n","    print(f\"  Min value: {np.min(embeddings):.6f}\")\n","    print(f\"  Max value: {np.max(embeddings):.6f}\")\n","    print(f\"  Mean: {np.mean(embeddings):.6f}\")\n","    print(f\"  Std: {np.std(embeddings):.6f}\")\n","\n","    # Check for problematic values\n","    nan_count = np.sum(np.isnan(embeddings))\n","    inf_count = np.sum(np.isinf(embeddings))\n","\n","    if nan_count > 0:\n","        print(f\"  ‚ö† NaN values: {nan_count}\")\n","    if inf_count > 0:\n","        print(f\"  ‚ö† Infinite values: {inf_count}\")\n","\n","    if nan_count == 0 and inf_count == 0:\n","        print(f\"  ‚úì No NaN or infinite values\")\n","\n","    # Suggest reshaping strategy\n","    if len(embeddings.shape) == 3:\n","        suggested_shape = (embeddings.shape[0], embeddings.shape[1] * embeddings.shape[2])\n","        print(f\"\\nSuggested reshape for 2D: {embeddings.shape} -> {suggested_shape}\")\n","    elif len(embeddings.shape) == 4:\n","        suggested_shape = (embeddings.shape[0], np.prod(embeddings.shape[1:]))\n","        print(f\"\\nSuggested reshape for 2D: {embeddings.shape} -> {suggested_shape}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"colab":{"provenance":[],"gpuType":"T4","runtime_attributes":{"runtime_version":"2025.07"}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}