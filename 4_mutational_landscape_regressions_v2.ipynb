{"cells":[{"cell_type":"markdown","id":"3db34e7c","metadata":{"id":"3db34e7c"},"source":["\n","# GB1 FITNESS PREDICTION WITH PROTEIN LANGUAGE MODEL EMBEDDINGS"]},{"cell_type":"markdown","id":"6f3a92ab","metadata":{"id":"6f3a92ab"},"source":["Dataset: GB1 domain from Wu et al. (2016) eLife\n","Reference: https://elifesciences.org/articles/16965\n","Data from FLIP benchmark: https://github.com/J-SNACKKB/FLIP\n","\n","This script demonstrates:\n","1. Loading multiple train/test splits from FLIP benchmark\n","2. Extracting embeddings using ESM-2 protein language model\n","3. Regression using two methods (Random Forest, Ridge)\n","4. Comparing performance across different splits\n","5. In silico screening to identify high-fitness variants"]},{"cell_type":"code","execution_count":1,"id":"626b6fe9","metadata":{"id":"626b6fe9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762612400282,"user_tz":0,"elapsed":39468,"user":{"displayName":"Sophia","userId":"12215712277586225497"}},"outputId":"9b153ee9-0d6a-434f-9fae-bcefe634649d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7a3725dbf070>"]},"metadata":{},"execution_count":1}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n","from sklearn.linear_model import Ridge\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n","import torch\n","from transformers import AutoTokenizer, EsmModel\n","import warnings\n","import gc\n","import copy\n","warnings.filterwarnings('ignore')\n","\n","# Set random seeds for reproducibility\n","np.random.seed(42)\n","torch.manual_seed(42)"]},{"cell_type":"code","source":["if 'google.colab' in str(get_ipython()):\n","    print(\"Running on Google Colab. Executing Colab-specific commands...\")\n","    # Mount Google Drive to access files\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    # Drive location for the fasta files\n","    data_loc = '/content/drive/MyDrive/AIDrivenDesignOfBiologics/AIDrivenDesignOfBiologics-PEGSEurope-2025/ProteinLanguageModels/pLM_basics_and_benchmarking/'\n","\n","else:\n","    print(\"Not running on Google Colab. Skipping Colab-specific commands.\")\n","    print(\"Running in a local environment or Jupyter Notebook.\")\n","    #data_loc = '/home/davidnannemann/AIDD4B/ProteinLMs/GB1'"],"metadata":{"id":"h9GpyzyhGKwR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762612441716,"user_tz":0,"elapsed":25117,"user":{"displayName":"Sophia","userId":"12215712277586225497"}},"outputId":"8b0e47a5-9ff3-4cce-94a2-fa7416ec0633"},"id":"h9GpyzyhGKwR","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Running on Google Colab. Executing Colab-specific commands...\n","Mounted at /content/drive\n"]}]},{"cell_type":"markdown","id":"41547fa6","metadata":{"id":"41547fa6"},"source":["### STEP 1: Load Data from Multiple Splits"]},{"cell_type":"code","execution_count":3,"id":"4a373396","metadata":{"id":"4a373396","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762612454798,"user_tz":0,"elapsed":689,"user":{"displayName":"Sophia","userId":"12215712277586225497"}},"outputId":"b5c55d6d-15c8-48c2-a4ff-22f016ea95a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["STEP 1: Loading GB1 Data from FLIP Benchmark\n","--------------------------------------------------------------------------------\n","✗ Could not load one_vs_rest from URL: [Errno 2] No such file or directory: '/content/drive/MyDrive/AIDrivenDesignOfBiologics/AIDrivenDesignOfBiologics-PEGSEurope-2025/ProteinLanguageModels/pLM_basics_and_benchmarking//GB1_data/splits/one_vs_rest.csv'\n","  Please download manually from: /content/drive/MyDrive/AIDrivenDesignOfBiologics/AIDrivenDesignOfBiologics-PEGSEurope-2025/ProteinLanguageModels/pLM_basics_and_benchmarking//GB1_data/splits/one_vs_rest.csv\n","✗ Could not load two_vs_rest from URL: [Errno 2] No such file or directory: '/content/drive/MyDrive/AIDrivenDesignOfBiologics/AIDrivenDesignOfBiologics-PEGSEurope-2025/ProteinLanguageModels/pLM_basics_and_benchmarking//GB1_data/splits/two_vs_rest.csv'\n","  Please download manually from: /content/drive/MyDrive/AIDrivenDesignOfBiologics/AIDrivenDesignOfBiologics-PEGSEurope-2025/ProteinLanguageModels/pLM_basics_and_benchmarking//GB1_data/splits/two_vs_rest.csv\n","✗ Could not load three_vs_rest from URL: [Errno 2] No such file or directory: '/content/drive/MyDrive/AIDrivenDesignOfBiologics/AIDrivenDesignOfBiologics-PEGSEurope-2025/ProteinLanguageModels/pLM_basics_and_benchmarking//GB1_data/splits/three_vs_rest.csv'\n","  Please download manually from: /content/drive/MyDrive/AIDrivenDesignOfBiologics/AIDrivenDesignOfBiologics-PEGSEurope-2025/ProteinLanguageModels/pLM_basics_and_benchmarking//GB1_data/splits/three_vs_rest.csv\n","✗ Could not load sampled from URL: [Errno 2] No such file or directory: '/content/drive/MyDrive/AIDrivenDesignOfBiologics/AIDrivenDesignOfBiologics-PEGSEurope-2025/ProteinLanguageModels/pLM_basics_and_benchmarking//GB1_data/splits/sampled.csv'\n","  Please download manually from: /content/drive/MyDrive/AIDrivenDesignOfBiologics/AIDrivenDesignOfBiologics-PEGSEurope-2025/ProteinLanguageModels/pLM_basics_and_benchmarking//GB1_data/splits/sampled.csv\n","\n"]}],"source":["def load_gb1_splits(base_dir=f\"{data_loc}/GB1_data/splits/\"):\n","    \"\"\"\n","    Load GB1 data from different split strategies.\n","\n","    Available splits:\n","    - one_vs_rest: Single mutants in train, rest in test\n","    - two_vs_rest: Single + double mutants in train, rest in test\n","    - three_vs_rest: Single + double + triple mutants in train, rest in test\n","    - sampled: Random 80/20 split\n","    \"\"\"\n","\n","    splits = ['one_vs_rest', 'two_vs_rest', 'three_vs_rest', 'sampled']\n","    all_data = {}\n","\n","    print(\"STEP 1: Loading GB1 Data from FLIP Benchmark\")\n","    print(\"-\" * 80)\n","\n","    for split in splits:\n","        try:\n","            # Try to load from URL\n","            split_path = f\"{base_dir}{split}.csv\"\n","            df = pd.read_csv(split_path)\n","            df['split_name'] = split\n","            all_data[split] = df\n","            print(f\"✓ Loaded {split}: {len(df)} sequences\")\n","            print(f\"  Train: {(df['set']=='train').sum()}, Test: {(df['set']=='test').sum()}\")\n","        except Exception as e:\n","            print(f\"✗ Could not load {split} from URL: {e}\")\n","            print(f\"  Please download manually from: {split_path}\")\n","\n","    # if not all_data:\n","    #     print(\"\\n⚠ No data loaded from URL. Creating demo dataset...\")\n","    #     all_data = create_demo_gb1_data()\n","\n","    print()\n","    return all_data\n","\n","all_splits = load_gb1_splits()"]},{"cell_type":"code","execution_count":4,"id":"44e3201d","metadata":{"id":"44e3201d","colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"status":"ok","timestamp":1762612465527,"user_tz":0,"elapsed":50,"user":{"displayName":"Sophia","userId":"12215712277586225497"}},"outputId":"161ecdab-408e-44ea-f411-2ac039872add"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Empty DataFrame\n","Columns: []\n","Index: []"],"text/html":["\n","  <div id=\"df-dd02e698-2cb4-4d1d-b042-cf82ce5e525a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd02e698-2cb4-4d1d-b042-cf82ce5e525a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-dd02e698-2cb4-4d1d-b042-cf82ce5e525a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-dd02e698-2cb4-4d1d-b042-cf82ce5e525a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","  <div id=\"id_a1b85829-f094-4be7-8001-4bbd6fa4a8a5\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('all_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_a1b85829-f094-4be7-8001-4bbd6fa4a8a5 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('all_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"all_df","summary":"{\n  \"name\": \"all_df\",\n  \"rows\": 0,\n  \"fields\": []\n}"}},"metadata":{},"execution_count":4}],"source":["all_df = pd.DataFrame()\n","\n","# there is probably a faster way to do this join, but figuring the syntax would take me longer than writing this function.\n","for split in all_splits:\n","    df = all_splits[split]\n","    #print(df.shape)\n","    for i, row in df.iterrows():\n","        if not row['sequence'] in all_df.index.values:\n","            all_df.at[row['sequence'],'sequence'] = row['sequence']\n","        all_df.at[row['sequence'],'target'] = row['target']\n","        all_df.at[row['sequence'],f\"{split} split set\"] = row['set']\n","\n","all_df"]},{"cell_type":"code","execution_count":5,"id":"c7971933","metadata":{"id":"c7971933","colab":{"base_uri":"https://localhost:8080/","height":291},"executionInfo":{"status":"error","timestamp":1762612471870,"user_tz":0,"elapsed":76,"user":{"displayName":"Sophia","userId":"12215712277586225497"}},"outputId":"f0bcc34d-0ed8-4e53-f0bc-31b8a5a59a5f"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Could not interpret value `target` for `x`. An entry with this name does not appear in `data`.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2332928301.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/distributions.py\u001b[0m in \u001b[0;36mhistplot\u001b[0;34m(data, x, y, hue, weights, stat, bins, binwidth, binrange, discrete, cumulative, common_bins, common_norm, multiple, element, fill, shrink, kde, kde_kws, line_kws, thresh, pthresh, pmax, cbar, cbar_ax, cbar_kws, palette, hue_order, hue_norm, color, log_scale, legend, ax, **kwargs)\u001b[0m\n\u001b[1;32m   1377\u001b[0m ):\n\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m     p = _DistributionPlotter(\n\u001b[0m\u001b[1;32m   1380\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m         \u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/distributions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    108\u001b[0m     ):\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# information for numeric axes would be information about log scales.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_ordered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# alt., used DefaultDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;31m# TODO Lots of tests assume that these are called to initialize the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/_base.py\u001b[0m in \u001b[0;36massign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0;31m# to centralize / standardize data consumption logic.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"long\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m             \u001b[0mplot_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlotData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/_core/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/_core/data.py\u001b[0m in \u001b[0;36m_assign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                     \u001b[0merr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"An entry with this name does not appear in `data`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Could not interpret value `target` for `x`. An entry with this name does not appear in `data`."]}],"source":["sns.histplot(all_df, x='target')"]},{"cell_type":"code","execution_count":6,"id":"70f8548a","metadata":{"id":"70f8548a","colab":{"base_uri":"https://localhost:8080/","height":362},"executionInfo":{"status":"error","timestamp":1762612477370,"user_tz":0,"elapsed":56,"user":{"displayName":"Sophia","userId":"12215712277586225497"}},"outputId":"e0d921f3-6145-4d1f-d6f5-0c3bc04fe53e"},"outputs":[{"output_type":"stream","name":"stdout","text":["activity fraction\tvariant count\tlibrary %\n"]},{"output_type":"error","ename":"KeyError","evalue":"'target'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3156469160.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"activity fraction\\tvariant count\\tlibrary %\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mactivity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0mactivity\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{activity:.2f} \\t\\t\\t{count} \\t\\t{count/all_df.shape[0]:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHashable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'target'"]}],"source":["# count the number of variants with activity less than x\n","print(f\"activity fraction\\tvariant count\\tlibrary %\")\n","for activity in [0.5,0.25,0.05, 0.02, 0.01, 0.0]:\n","    count = all_df.loc[all_df['target']<=activity].shape[0]\n","    print(f\"{activity:.2f} \\t\\t\\t{count} \\t\\t{count/all_df.shape[0]:.3f}\")"]},{"cell_type":"markdown","id":"17861553","metadata":{"id":"17861553"},"source":["### STEP 2: Extract ESM-1b Embeddings"]},{"cell_type":"code","execution_count":null,"id":"4eeb0402","metadata":{"id":"4eeb0402"},"outputs":[],"source":["def extract_esm_embeddings(sequences, model_name=\"facebook/esm2_t33_650M_UR50D\"):\n","    \"\"\"\n","    Extract embeddings using ESM-2 model.\n","\n","    ESM-2 is the latest version of ESM and performs well on fitness prediction.\n","    Using 650M parameter version for balance of performance and speed.\n","    \"\"\"\n","\n","    print(\"\\nSTEP 2: Extracting ESM-2 Embeddings\")\n","    print(\"-\" * 80)\n","    print(f\"Model: {model_name}\")\n","    print(f\"Sequences to embed: {len(sequences)}\")\n","\n","    # Load model and tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = EsmModel.from_pretrained(model_name)\n","    model.eval()\n","\n","    # Move to GPU if available\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","    print(f\"Using device: {device}\")\n","\n","    embeddings = []\n","\n","    # Process sequences in batches\n","    batch_size = 8\n","    with torch.no_grad():\n","        for i in range(0, len(sequences), batch_size):\n","            batch = sequences[i:i+batch_size]\n","\n","            # Tokenize\n","            inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","            # Get embeddings\n","            outputs = model(**inputs)\n","\n","            # Use mean pooling over sequence length\n","            attention_mask = inputs['attention_mask']\n","            token_embeddings = outputs.last_hidden_state\n","\n","            # Mean pooling\n","            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n","            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","            batch_embeddings = sum_embeddings / sum_mask\n","\n","            embeddings.append(batch_embeddings.cpu().numpy())\n","\n","            # Clear intermediate tensors\n","            del inputs, outputs, token_embeddings, batch_embeddings\n","\n","            if (i + batch_size) % 40 == 0:\n","                print(f\"  Processed {min(i+batch_size, len(sequences))}/{len(sequences)} sequences\")\n","\n","    embeddings = np.vstack(embeddings)\n","    print(f\"✓ Embedding shape: {embeddings.shape}\")\n","\n","    del model, tokenizer\n","    gc.collect()\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","    print()\n","\n","    return embeddings\n","\n","embeddings = extract_esm_embeddings(all_df['sequence'].tolist())\n"]},{"cell_type":"markdown","id":"be131c6f","metadata":{"id":"be131c6f"},"source":["### STEP 3: Train Regression Models"]},{"cell_type":"code","execution_count":null,"id":"bab3f6b3","metadata":{"id":"bab3f6b3"},"outputs":[],"source":["def train_and_evaluate(X_train, y_train, X_test, y_test, split_name):\n","    \"\"\"\n","    Train two regression models and evaluate performance.\n","\n","    Models:\n","    1. Random Forest: Non-linear, captures complex patterns\n","    2. Ridge Regression: Linear, fast and interpretable\n","    \"\"\"\n","\n","    # Standardize features\n","    scaler = StandardScaler()\n","    X_train_scaled = scaler.fit_transform(X_train)\n","    X_test_scaled = scaler.transform(X_test)\n","\n","    results = {}\n","\n","    # Model 1: Random Forest\n","    print(\"Training Random Forest Regression Model\")\n","    rf_model = RandomForestRegressor(\n","        n_estimators=100,\n","        max_depth=10,\n","        random_state=42,\n","        n_jobs=-1\n","    )\n","    rf_model.fit(X_train_scaled, y_train)\n","    rf_pred = rf_model.predict(X_test_scaled)\n","\n","    results['Random Forest'] = {\n","        'model': rf_model,\n","        'scaler': scaler,\n","        'predictions': rf_pred,\n","        'mse': mean_squared_error(y_test, rf_pred),\n","        'mae': mean_absolute_error(y_test, rf_pred),\n","        'r2': r2_score(y_test, rf_pred),\n","        'spearman': pd.Series(y_test).corr(pd.Series(rf_pred), method='spearman')\n","    }\n","\n","    # Model 2: Ridge Regression\n","    print(\"Training Ridge Regression Model\")\n","    ridge_model = Ridge(alpha=1.0, random_state=42)\n","    ridge_model.fit(X_train_scaled, y_train)\n","    ridge_pred = ridge_model.predict(X_test_scaled)\n","\n","    results['Ridge'] = {\n","        'model': ridge_model,\n","        'scaler': scaler,\n","        'predictions': ridge_pred,\n","        'mse': mean_squared_error(y_test, ridge_pred),\n","        'mae': mean_absolute_error(y_test, ridge_pred),\n","        'r2': r2_score(y_test, ridge_pred),\n","        'spearman': pd.Series(y_test).corr(pd.Series(ridge_pred), method='spearman')\n","    }\n","\n","    return results\n"]},{"cell_type":"markdown","id":"e07050d3","metadata":{"id":"e07050d3"},"source":["### STEP 4: Iterate Over All Splits"]},{"cell_type":"code","execution_count":null,"id":"0f2f3403","metadata":{"id":"0f2f3403"},"outputs":[],"source":["# Store results\n","all_results = []\n","split_predictions = {}\n","\n","for split in ['one_vs_rest', 'two_vs_rest', 'three_vs_rest', 'sampled']:\n","\n","    print(f\"\\nProcessing split: {split}\")\n","    print(\"  \" + \"-\" * 76)\n","\n","    # Split data\n","    train_mask = all_df[f'{split} split set'] == 'train'\n","    test_mask  = all_df[f'{split} split set'] == 'test'\n","\n","    X_train = embeddings[train_mask]\n","    y_train = all_df.loc[train_mask, 'target'].values\n","    X_test = embeddings[test_mask]\n","    y_test = all_df.loc[test_mask, 'target'].values\n","\n","    print(f\"  Train size: {len(X_train)}, Test size: {len(X_test)}\")\n","\n","    # Train and evaluate\n","    results = train_and_evaluate(X_train, y_train, X_test, y_test, split)\n","\n","    # Store results\n","    for model_name, metrics in results.items():\n","        try:\n","            result_dict = {\n","                'Split': split,\n","                'Model': model_name,\n","                'R²': metrics['r2'],\n","                'Spearman': metrics['spearman'],\n","                'MAE': metrics['mae'],\n","                'RMSE': np.sqrt(metrics['mse'])\n","            }\n","        except TypeError:\n","            result_dict = {\n","                'Split': split,\n","                'Model': model_name,\n","                'R²': metrics['r2'],\n","                'Spearman': metrics['spearman'],\n","                'MAE': metrics['mae'],\n","                'RMSE': 0.0\n","            }\n","\n","        # Add classification metrics for two-stage model\n","        if model_name == 'Two-Stage':\n","            result_dict['Class_Acc'] = metrics['classification_acc']\n","            result_dict['Class_AUC'] = metrics['classification_auc']\n","\n","        all_results.append(result_dict)\n","\n","    # Store for plotting\n","    split_predictions[split] = {\n","        'y_test': y_test,\n","        'rf_pred': results['Random Forest']['predictions'],\n","        'ridge_pred': results['Ridge']['predictions']\n","    }\n","\n","    print(f\"  ✓ Random Forest - R²: {results['Random Forest']['r2']:.3f}, Spearman: {results['Random Forest']['spearman']:.3f}\")\n","    print(f\"  ✓ Ridge         - R²: {results['Ridge']['r2']:.3f}, Spearman: {results['Ridge']['spearman']:.3f}\")\n","\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"id":"7005240f","metadata":{"id":"7005240f"},"outputs":[],"source":["results_df = pd.DataFrame(all_results)\n","print(\"\\nPerformance Metrics:\")\n","print(results_df.to_string(index=False))\n","\n","# Save to CSV\n","# results_df.to_csv('gb1_results_summary.csv', index=False)"]},{"cell_type":"markdown","id":"5f859eeb","metadata":{"id":"5f859eeb"},"source":["### STEP 5: Visualizations"]},{"cell_type":"code","execution_count":null,"id":"a807683f","metadata":{"id":"a807683f"},"outputs":[],"source":["# Plot 1: Performance comparison across splits\n","fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","# Spearman correlation\n","for model in ['Random Forest', 'Ridge']: #, 'XGBoost', 'Two-Stage'\n","    model_data = results_df[results_df['Model'] == model]\n","    axes[0].plot(model_data['Split'], model_data['Spearman'],\n","                 marker='o', linewidth=2, markersize=8, label=model)\n","\n","axes[0].set_xlabel('Split Strategy', fontsize=12)\n","axes[0].set_ylabel('Spearman Correlation', fontsize=12)\n","axes[0].set_title('Model Performance Across Splits', fontsize=14, fontweight='bold')\n","axes[0].legend()\n","axes[0].grid(True, alpha=0.3)\n","axes[0].tick_params(axis='x', rotation=45)\n","\n","# R² score\n","for model in ['Random Forest', 'Ridge']: # , 'XGBoost', 'Two-Stage'\n","    model_data = results_df[results_df['Model'] == model]\n","    axes[1].plot(model_data['Split'], model_data['R²'],\n","                 marker='s', linewidth=2, markersize=8, label=model)\n","\n","axes[1].set_xlabel('Split Strategy', fontsize=12)\n","axes[1].set_ylabel('R² Score', fontsize=12)\n","axes[1].set_title('R² Performance Across Splits', fontsize=14, fontweight='bold')\n","axes[1].legend()\n","axes[1].grid(True, alpha=0.3)\n","axes[1].tick_params(axis='x', rotation=45)\n","\n","plt.tight_layout()\n","plt.savefig('gb1_split_comparison.png', dpi=300, bbox_inches='tight')\n","print(\"✓ Saved: gb1_split_comparison.png\")\n","\n","# Plot 2: Predicted vs Actual for each split\n","n_splits = len(split_predictions)\n","fig, axes = plt.subplots(3, n_splits, figsize=(5*n_splits, 10))\n","if n_splits == 1:\n","    axes = axes.reshape(-1, 1)\n","\n","for idx, (split_name, preds) in enumerate(split_predictions.items()):\n","    y_test = preds['y_test']\n","\n","    # Random Forest\n","    axes[0, idx].scatter(y_test, preds['rf_pred'], alpha=0.6, s=50, edgecolors='black')\n","    axes[0, idx].plot([y_test.min(), y_test.max()],\n","                      [y_test.min(), y_test.max()],\n","                      'r--', lw=2, label='Perfect')\n","    axes[0, idx].set_xlabel('Actual Fitness', fontsize=11)\n","    axes[0, idx].set_ylabel('Predicted Fitness', fontsize=11)\n","    axes[0, idx].set_title(f'{split_name}\\nRandom Forest', fontsize=12, fontweight='bold')\n","    axes[0, idx].grid(True, alpha=0.3)\n","    axes[0, idx].legend()\n","\n","    # Ridge\n","    axes[1, idx].scatter(y_test, preds['ridge_pred'], alpha=0.6, s=50,\n","                         edgecolors='black', color='orange')\n","    axes[1, idx].plot([y_test.min(), y_test.max()],\n","                      [y_test.min(), y_test.max()],\n","                      'r--', lw=2, label='Perfect')\n","    axes[1, idx].set_xlabel('Actual Fitness', fontsize=11)\n","    axes[1, idx].set_ylabel('Predicted Fitness', fontsize=11)\n","    axes[1, idx].set_title(f'{split_name}\\nRidge Regression', fontsize=12, fontweight='bold')\n","    axes[1, idx].grid(True, alpha=0.3)\n","    axes[1, idx].legend()\n","\n","    # Comparison of Ridge and Random Forest\n","    axes[2, idx].scatter(preds['rf_pred'], preds['ridge_pred'], alpha=0.6, s=50,\n","                         edgecolors='black', color='green')\n","    axes[2, idx].plot([y_test.min(), y_test.max()],\n","                      [y_test.min(), y_test.max()],\n","                      'r--', lw=2)\n","    axes[2, idx].set_xlabel('Predicted Fitness (Random Forest)', fontsize=11)\n","    axes[2, idx].set_ylabel('Predicted Fitness (Ridge)', fontsize=11)\n","    axes[2, idx].set_title(f'{split_name}\\nRegression Comparison', fontsize=12, fontweight='bold')\n","    axes[2, idx].grid(True, alpha=0.3)\n","    axes[2, idx].legend()\n","\n","plt.tight_layout()\n","plt.savefig('gb1_predictions_by_split.png', dpi=300, bbox_inches='tight')\n","print(\"✓ Saved: gb1_predictions_by_split.png\")\n","\n","plt.show()\n"]},{"cell_type":"markdown","id":"06999bcc","metadata":{"id":"06999bcc"},"source":["Examine these results and consider the following.\n","\n","- How does the amount or character of the training data effect the quality of the predicted fitness?\n","   - Which splitting method shows highest performance?\n","   - Is there a potential for data leakage in some of the splitting methods?\n","   - Which method for generating training data is most practical from a wet-lab perspective?\n","- What aspects of the data might hinder model training?\n","    - How are non-functional variants differentiated in a regression? Zero is zero but some mutations will be more disruptive.\n","    - Could you improve the quality of training by removing non-functional variants? Or using a two-stage method that separates 'functional' classification from 'how functional' regression?\n","- Would an ensemble method be useful for picking variants for the next round of screening?"]},{"cell_type":"markdown","id":"2e3331de","metadata":{"id":"2e3331de"},"source":["### STEP 6: In Silico Screening - Find High-Fitness Variants"]},{"cell_type":"code","execution_count":null,"id":"9739488b","metadata":{"id":"9739488b"},"outputs":[],"source":["def generate_mutant_library(wt_sequence, positions_to_mutate, amino_acids='ACDEFGHIKLMNPQRSTVWY',\n","                            double_mutant=False, triple_mutant=False, random_mutant=False,\n","                            max_mutants=1000):\n","    \"\"\"\n","    Generate a library of single and double mutants at specified positions.\n","\n","    Parameters:\n","    -----------\n","    wt_sequence : str\n","        Wild-type protein sequence\n","    positions_to_mutate : list of int\n","        Positions (0-indexed) to introduce mutations\n","    amino_acids : str\n","        Amino acids to consider for mutations\n","\n","    Returns:\n","    --------\n","    variants : list of dict\n","        List of variant information with sequences and mutation descriptions\n","    \"\"\"\n","\n","    variant_seqs = set() # Use a set to ensure no duplicates\n","    variants = []\n","\n","    # Add wild-type\n","    variant_seqs.add(wt_sequence)\n","    variants.append({\n","        'sequence': wt_sequence,\n","        'mutation': 'WT',\n","        'n_mutations': 0\n","    })\n","\n","    # Generate single mutants\n","    for pos in positions_to_mutate:\n","        wt_aa = wt_sequence[pos]\n","        for aa in amino_acids:\n","            if aa != wt_aa:\n","                mutant_seq = wt_sequence[:pos] + aa + wt_sequence[pos+1:]\n","                if mutant_seq in variant_seqs: continue\n","                variant_seqs.add(mutant_seq)\n","                variants.append({\n","                    'sequence': mutant_seq,\n","                    'mutation': f'{wt_aa}{pos+1}{aa}',\n","                    'n_mutations': 1\n","                })\n","\n","    if double_mutant:\n","        # Generate double mutants (optional - can be computationally expensive)\n","        for i, pos1 in enumerate(positions_to_mutate):\n","            wt_aa1 = wt_sequence[pos1]\n","            for pos2 in positions_to_mutate[i+1:]:\n","                wt_aa2 = wt_sequence[pos2]\n","                for aa1 in amino_acids:\n","                    if aa1 != wt_aa1:\n","                        for aa2 in amino_acids:\n","                            if aa2 != wt_aa2:\n","                                mutant_seq = wt_sequence[:pos1] + aa1 + wt_sequence[pos1+1:]\n","                                mutant_seq = mutant_seq[:pos2] + aa2 + mutant_seq[pos2+1:]\n","                                if mutant_seq in variant_seqs: continue\n","                                variant_seqs.add(mutant_seq)\n","                                variants.append({\n","                                    'sequence': mutant_seq,\n","                                    'mutation': f'{wt_aa1}{pos1+1}{aa1}/{wt_aa2}{pos2+1}{aa2}',\n","                                    'n_mutations': 2\n","                                })\n","\n","    if triple_mutant:\n","    # Generate triple mutants (optional - can be computationally expensive)\n","        for i, pos1 in enumerate(positions_to_mutate):\n","            wt_aa1 = wt_sequence[pos1]\n","            for pos2 in positions_to_mutate[i+1:]:\n","                wt_aa2 = wt_sequence[pos2]\n","                for pos3 in positions_to_mutate[i+2:]:\n","                    wt_aa3 = wt_sequence[pos3]\n","                    for aa1 in amino_acids:\n","                        if aa1 != wt_aa1:\n","                            for aa2 in amino_acids:\n","                                if aa2 != wt_aa2:\n","                                    for aa3 in amino_acids:\n","                                        if aa3 != wt_aa3:\n","                                            mutant_seq = wt_sequence[:pos1] + aa1 + wt_sequence[pos1+1:]\n","                                            mutant_seq = mutant_seq[:pos2] + aa2 + mutant_seq[pos2+1:]\n","                                            mutant_seq = mutant_seq[:pos3] + aa3 + mutant_seq[pos3+1:]\n","                                            if mutant_seq in variant_seqs: continue\n","                                            variant_seqs.add(mutant_seq)\n","                                            variants.append({\n","                                                'sequence': mutant_seq,\n","                                                'mutation': f'{wt_aa1}{pos1+1}{aa1}/{wt_aa2}{pos2+1}{aa2}/{wt_aa3}{pos3+1}{aa3}',\n","                                                'n_mutations': 3\n","                                            })\n","\n","    if random_mutant:\n","    # Generate random mutations (optional - can be computationally expensive)\n","        n=0\n","        #while n<10:\n","        while len(variant_seqs)<max_mutants:\n","            mutant_seq = copy.deepcopy(wt_sequence) # deep copy the wild-type sequence so mutant_seq is not just a pointer to the wt_sequence object\n","\n","            # pick the positions that are to be mutated. If not specified, a very high probability of variants mutated at every position are created\n","            mutate_positions = [np.random.rand()<0.5 for pos in positions_to_mutate] # decrease value to increase ratio of multi-mutants\n","\n","            mutant_str = [] # track the \"name\" of each mutated position\n","\n","            # create the mutated sequence\n","            for mutate, pos in zip(mutate_positions, positions_to_mutate):\n","                if mutate:\n","                    wt_aa = wt_sequence[pos]\n","                    mut_aa = np.random.choice(list(amino_acids)) # pick a random amino acid\n","                    mutant_seq = mutant_seq[:pos] + mut_aa + mutant_seq[pos+1:]\n","                    mutant_str.append(f\"{wt_aa}{pos+1}{mut_aa}\")\n","            if mutant_seq in variant_seqs: continue\n","            variant_seqs.add(mutant_seq)\n","            variants.append({\n","                'sequence': mutant_seq,\n","                'mutation': \"/\".join(mutant_str),\n","                'n_mutations': sum(mutate_positions)\n","            })\n","            n+=1\n","\n","    return variants\n","\n","# wt_sequence = \"MQYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDDATKTFTVTELEVLFQGPLDPNSMATYEVLCEVARKLGTDDREVVLFLLNVFIPQPTLAQLIGALRALKEEGRLTFPLLAECLFRAGRRDLLRDLLHLDPRFLERHLAGTMSYFSPYQLTVLHVDGELCARDIRSLIFLSKDTIGSRSTPQTFLHWVYCMENLDLLGPTDVDALMSMLRSLSRVDLQRQVQTLMGLHLSGPSHSQHYRHTPLEHHHHHH\"\n","# positions_to_mutate = [38, 39, 40, 53]\n","# variants = generate_mutant_library(wt_sequence, positions_to_mutate,\n","#                                    double_mutant=False, triple_mutant=False, random_mutant=True)\n","# print(variants)"]},{"cell_type":"code","execution_count":null,"id":"7646ec81","metadata":{"id":"7646ec81"},"outputs":[],"source":["def screen_variants_for_high_fitness(model, scaler, known_sequences, wt_sequence,\n","                                     positions_to_mutate, top_n=20, batch_size=40,\n","                                     double_mutant=False, triple_mutant=False, random_mutant=False,\n","                                     max_mutants=1000):\n","    \"\"\"\n","    In silico screening to identify high-fitness variants not in training set.\n","\n","    This function demonstrates how to use the trained model for variant discovery:\n","    1. Generate a library of variants\n","    2. Filter out known sequences\n","    3. Extract embeddings for all variants\n","    4. Predict fitness using trained model\n","    5. Return top predicted variants for experimental validation\n","\n","    Parameters:\n","    -----------\n","    model : trained model\n","        The regression model to use for predictions\n","    scaler : StandardScaler or None\n","        Feature scaler (if applicable)\n","    known_sequences : set\n","        Set of sequences already characterized (to exclude)\n","    wt_sequence : str\n","        Wild-type sequence\n","    positions_to_mutate : list of int\n","        Positions to mutate (0-indexed)\n","    top_n : int\n","        Number of top candidates to return\n","    batch_size : int\n","        Batch size for embedding extraction\n","\n","    Returns:\n","    --------\n","    candidates_df : DataFrame\n","        Top predicted variants with predicted fitness scores\n","    \"\"\"\n","\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"IN SILICO SCREENING FOR HIGH-FITNESS VARIANTS\")\n","    print(\"=\"*80)\n","    print()\n","\n","    # Step 1: Generate variant library\n","    print(\"Generating variant library...\")\n","    variants = generate_mutant_library(wt_sequence, positions_to_mutate,\n","                                       double_mutant=double_mutant, triple_mutant=triple_mutant,\n","                                       random_mutant=random_mutant, max_mutants=max_mutants)\n","    print(f\"✓ Generated {len(variants)} variants\")\n","\n","    # Step 2: Filter out known sequences\n","    novel_variants = [v for v in variants if v['sequence'] not in known_sequences]\n","    print(f\"✓ {len(novel_variants)} novel variants (not in training/test set)\")\n","\n","    if len(novel_variants) == 0:\n","        print(\"⚠ All variants are already characterized!\")\n","        return pd.DataFrame()\n","\n","    # Step 3: Extract embeddings\n","    print(\"\\nExtracting embeddings for variant library...\")\n","    sequences = [v['sequence'] for v in novel_variants]\n","    embeddings = extract_esm_embeddings(sequences)\n","\n","    # Step 4: Make predictions\n","    print(\"\\nPredicting fitness for novel variants...\")\n","    if scaler is not None:\n","        embeddings_scaled = scaler.transform(embeddings)\n","    else:\n","        embeddings_scaled = embeddings\n","\n","    predictions = model.predict(embeddings_scaled)\n","\n","    # Step 5: Create results DataFrame\n","    candidates_df = pd.DataFrame(novel_variants)\n","    candidates_df['predicted_fitness'] = predictions\n","\n","    # Step 6: Sort by predicted fitness and select top candidates\n","    candidates_df = candidates_df.sort_values('predicted_fitness', ascending=False)\n","    top_candidates = candidates_df.head(top_n)\n","\n","    print(f\"\\n✓ Top {top_n} predicted high-fitness variants:\")\n","    print(\"-\" * 80)\n","\n","    display_cols = ['mutation', 'predicted_fitness', 'n_mutations']\n","    if 'functionality_prob' in top_candidates.columns:\n","        display_cols.append('functionality_prob')\n","\n","    print(top_candidates[display_cols].to_string(index=False))\n","\n","    # Save to file\n","    candidates_df.to_csv('gb1_screening_results.csv', index=False)\n","    print(f\"\\n✓ Full screening results saved to: gb1_screening_results.csv\")\n","    print(f\"  (Total novel variants screened: {len(candidates_df)})\")\n","\n","    return top_candidates\n"]},{"cell_type":"code","execution_count":null,"id":"7d7726fa","metadata":{"id":"7d7726fa"},"outputs":[],"source":["# Get all known sequences\n","known_sequences = set(df['sequence'].tolist())\n","print(f\"Known sequences in dataset: {len(known_sequences)}\")\n","\n","# Define wild-type and positions to mutate\n","# GB1 wild-type at key positions\n","wt_sequence = \"MQYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDDATKTFTVTELEVLFQGPLDPNSMATYEVLCEVARKLGTDDREVVLFLLNVFIPQPTLAQLIGALRALKEEGRLTFPLLAECLFRAGRRDLLRDLLHLDPRFLERHLAGTMSYFSPYQLTVLHVDGELCARDIRSLIFLSKDTIGSRSTPQTFLHWVYCMENLDLLGPTDVDALMSMLRSLSRVDLQRQVQTLMGLHLSGPSHSQHYRHTPLEHHHHHH\"\n","\n","# Focus on the 4 positions from the Wu et al. study (0-indexed)\n","# Positions 38, 39, 40, 53 (V39, D40, G41, V54 in paper notation)\n","positions_to_mutate = [38, 39, 40, 53]\n","\n","print(f\"Wild-type sequence length: {len(wt_sequence)}\")\n","print(f\"Positions to mutate: {positions_to_mutate}\")\n","print(f\"WT amino acids at these positions: {[wt_sequence[p] for p in positions_to_mutate]}\")\n"]},{"cell_type":"code","execution_count":null,"id":"02b1ab0e","metadata":{"id":"02b1ab0e"},"outputs":[],"source":["# Re-train model on full dataset for best predictions\n","print(\"\\nRetraining Ridge model on full dataset...\")\n","X_all = embeddings\n","y_all = all_df['target'].values\n","\n","scaler = StandardScaler()\n","X_all_scaled = scaler.fit_transform(X_all)\n","\n","final_model = Ridge(alpha=1.0, random_state=42)\n","final_model.fit(X_all_scaled, y_all)\n","print(\"✓ Model trained on full dataset\")\n"]},{"cell_type":"code","execution_count":null,"id":"b7bc69f1","metadata":{"id":"b7bc69f1"},"outputs":[],"source":["# Perform screening\n","top_candidates = screen_variants_for_high_fitness(\n","    model=final_model,\n","    scaler=scaler,\n","    known_sequences=known_sequences,\n","    wt_sequence=wt_sequence,\n","    positions_to_mutate=positions_to_mutate,\n","    top_n=100,\n","    double_mutant=True, triple_mutant=False, random_mutant=True,\n","    max_mutants=1500 #50,000 provides a high-quality prediction but takes a while to run\n",")"]},{"cell_type":"markdown","id":"c60699d5","metadata":{"id":"c60699d5"},"source":["#### Visualization of Screening Results\n"]},{"cell_type":"code","execution_count":null,"id":"e93b7e18","metadata":{"id":"e93b7e18"},"outputs":[],"source":["if len(top_candidates) > 0:\n","    fig, axes = plt.subplots(1, 1, figsize=(7, 5))\n","\n","    # Plot 1: Top candidates by mutation type\n","    mutation_counts = top_candidates.groupby('n_mutations')['predicted_fitness'].apply(list)\n","\n","    positions = []\n","    fitness_values = []\n","    labels = []\n","\n","    for n_mut, fitnesses in mutation_counts.items():\n","        positions.extend([n_mut] * len(fitnesses))\n","        fitness_values.extend(fitnesses)\n","        labels.append(f'{n_mut} mutation(s)')\n","\n","    axes.scatter(positions, fitness_values, s=100, alpha=0.6, edgecolors='black')\n","    axes.set_xlabel('Number of Mutations', fontsize=12)\n","    axes.set_ylabel('Predicted Fitness', fontsize=12)\n","    axes.set_title('Top Candidates by Mutation Count', fontsize=14, fontweight='bold')\n","    axes.grid(True, alpha=0.3)\n","    axes.set_xticks(sorted(top_candidates['n_mutations'].unique()))\n","\n","    plt.tight_layout()\n","    plt.savefig('gb1_screening_visualization.png', dpi=300, bbox_inches='tight')\n","    print(\"✓ Saved: gb1_screening_visualization.png\")\n","    plt.show()"]},{"cell_type":"markdown","id":"ab6d542a","metadata":{"id":"ab6d542a"},"source":["**Discussion points**\n","- In a system with four mutation sites there are 160,000 variants. Is it possible to make a prediction for all of these?\n","- What alternative algorithms might be used to generate variants with the greatest predicted fitness?\n","    - Would a genetic algorithm be a practical method to traverse the sequence space?\n"]},{"cell_type":"markdown","id":"191ce19a","metadata":{"id":"191ce19a"},"source":["# Regression vs Fine-tuning"]},{"cell_type":"markdown","id":"eaed94ea","metadata":{"id":"eaed94ea"},"source":["Think about the difference between regression, performed here, and model fine-tuning. Explore when each is practical. Think about opportunities and pitfalls of each.\n","\n","The Gray Lab at Johns Hopkins has prepared the _Fitness Landscape for Antibodies (FLAb)_ database of antibody developability characteristics and scripts for fine-tuning of pLMs. Many of the datasets are behind non-commercial licenses, unfortunately, but some are permissively licensed. ([FLAb repository](https://github.com/Graylab/FLAb)).\n","\n","On your own, explore the zero-shot prediction methods in the scripts."]},{"cell_type":"markdown","id":"633b3cd7","metadata":{"id":"633b3cd7"},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}